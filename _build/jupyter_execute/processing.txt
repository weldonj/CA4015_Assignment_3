from google.colab import drive
drive.mount('/content/drive')

# You may have a different directory you want to change to...
import os
os.chdir('/content/drive/My Drive/CA4015/sleep_classify')

import numpy as np

x_clean = np.loadtxt('./outputs/cropped/46343_cleaned_motion.out')
print(x_clean.shape)

x_raw = np.loadtxt('./data/motion/46343_acceleration.txt')
print(x_raw.shape)

import matplotlib.pyplot as plt

sampfrom = 0
sampto = 85000 # Pick any value

plt.figure()
plt.plot(x_clean[sampfrom:sampto,0],x_clean[sampfrom:sampto,1], label='x')
plt.plot(x_clean[sampfrom:sampto,0],x_clean[sampfrom:sampto,2], label='y')
plt.plot(x_clean[sampfrom:sampto,0],x_clean[sampfrom:sampto,3], label='z')
plt.legend()
plt.title('Filtered Movement Data')
plt.show()

plt.figure()
plt.plot(x_raw[sampfrom:sampto,0],x_raw[sampfrom:sampto,1], label='x')
plt.plot(x_raw[sampfrom:sampto,0],x_raw[sampfrom:sampto,2], label='y')
plt.plot(x_raw[sampfrom:sampto,0],x_raw[sampfrom:sampto,3], label='z')
plt.legend()
plt.title('Raw Movement Data')
plt.show()

# Load the labels for one subject
labels = np.loadtxt('./data/labels/46343_labeled_sleep.txt')
print(labels.shape)
np.unique(labels[:,1])

# Show me a sample of the labels
print(labels[520:530,:])

# Get a set of patient ids
FULL_SET = []

import os

for filename in os.listdir("./outputs/cropped/"):
  patient_id = filename.split("_")[0]
  if patient_id not in FULL_SET: 
        FULL_SET.append(patient_id)

print(FULL_SET)

# choose what features we will use
feature_set = {"motion" : True, "hr" : True, "counts" : False}

import numpy as np

# we have 850,024 motion readings for this patient
motion_clean = np.loadtxt('./outputs/cropped/46343_cleaned_motion.out')
print(motion_clean.shape)
print(motion_clean[:10,:])

# we have 3,301 HR readings for the same patient
hr_clean = np.loadtxt('./outputs/cropped/46343_cleaned_hr.out')
print(hr_clean.shape)

# Load the labels for one subject
labels = np.loadtxt('./data/labels/46343_labeled_sleep.txt')
print(labels.shape)

# Show me a sample of the labels
print(labels[520:530,:])

# the index isn't the same for every file so we will need to calculate this for each patient
print("First 10 label times (sec): ", labels[:10,0])
print("Index of Motion data where time less than or equal to 30: ", np.max(np.nonzero(motion_clean[:,0] <= 30)))
print("Index of HR data where time less than or equal to 30: ", np.max(np.nonzero(hr_clean[:,0] <= 30)))

# we need to chunk motion and hr data into 30 sec block so it aligns with labels

def groupedAvg(myArray, N):
    myArray = myArray[:, 1:] # remove time column
    cum = np.cumsum(myArray,0)
    result = cum[N-1::N]/float(N)
    result[1:] = result[1:] - result[:-1]

    remainder = myArray.shape[0] % N
    if remainder != 0:
        if remainder < myArray.shape[0]:
            lastAvg = (cum[-1]-cum[-1-remainder])/float(remainder)
        else:
            lastAvg = cum[-1]/float(remainder)
        result = np.vstack([result, lastAvg])

    return result

chunked = groupedAvg(motion_clean, N=1503)
print(chunked[0:3])
# remember we have 567 sleep labels for subject 46343
# i think this is missing the first reading, because it takes the first N values to
# represent the 30sec bin, not 0 seconds
len(chunked)

# this looks like it works above but from the length below we see it doesn't match up.
# thats because the hr gets 5 readings in one 30sec period, 6 in the next, 5 in the next, etc...
# it doesn't seem to give consistent number of readings every 30sec
# so we'll need to check it every bin and change bin N to get every value
hr_blocked = groupedAvg(hr_clean, N=5)
print(hr_blocked[:3])
len(hr_blocked)

# this function will take the subject id, and feature set chosen by us
# it will open the feature file and group the readings into 30sec intervals.
# the output is a subject feature dictionary, with "key" : "value" being
# <feature> : <pandas dataframe containing feature values that match label times>

import pandas as pd

def interpolate_features(subject_id, feature_set, method="linear"):
  subject_features = dict()

  labels = pd.read_table('./data/labels/{}_labeled_sleep.txt'.format(subject_id),
                         header=None, delim_whitespace=True)
  
  bin_index = pd.Index(pd.np.arange(0.00, labels.iloc[-1,0]+30, 30.0))

  print("\nlength of subject {}'s labels file: {}\n".format(subject_id, len(labels)))

  for feature in feature_set:

    if feature_set[feature]:

      print("Interpolating {} feature for subject {}\n".format(feature, subject_id))

      df = pd.read_table('./outputs/cropped/{}_cleaned_{}.out'.format(subject_id, feature),
                         header=None, delim_whitespace=True, index_col=0)

      df = df.reindex(df.index.union(bin_index))
      df = df.interpolate(method=method, limit_direction='both')
      df = df.loc[df.index % 30 == 0]

      subject_features[feature] = df

  return subject_features, labels


subject_id = 46343 #1066528 #7749105 #2598705 #4018081 #1360686 #8686948 #844359 #5383425 

sub_feats, labels = interpolate_features(subject_id, feature_set)

print(sub_feats)

# these functions will allow us to plot our interpolated features

def plot_hr_df(sub_feats, sampfrom=0, sampto=85000):
    plt.figure(figsize=(15,7))
    feature = "hr"

    plt.plot(sub_feats[feature].loc[sampfrom:sampto].index,sub_feats[feature].iloc[sampfrom:sampto,0])
    plt.xlabel("Time (sec)")
    plt.ylabel("HR (bpm)")

    plt.title('Downsampled HR data')
    plt.show()


def plot_motion_df(sub_feats, sampfrom=0, sampto=85000):
    plt.figure(figsize=(15,7))
    feature = "motion"

    plt.plot(sub_feats[feature].loc[sampfrom:sampto].index,sub_feats[feature].iloc[sampfrom:sampto,0], label='x')
    plt.plot(sub_feats[feature].loc[sampfrom:sampto].index,sub_feats[feature].iloc[sampfrom:sampto,1], label='y')
    plt.plot(sub_feats[feature].loc[sampfrom:sampto].index,sub_feats[feature].iloc[sampfrom:sampto,2], label='z')
    plt.legend()
    plt.xlabel("Time (sec)")
    plt.ylabel("Motion")

    plt.title('Downsampled Motion data')
    plt.show()

plot_hr_df(sub_feats)
plot_motion_df(sub_feats)

print(labels.head())
labels.index*30

sub_feats["hr"].loc[:510]

# the times at which label is -1 or 4. We want to remove these time instances from the training data
# because we do not know what class it is

def remove_noise(subject_feats, labels):

    #noise_index = labels.loc[labels[1] == -1].index
    print(labels.columns)
    noise_index = labels.loc[labels[1].isin([-1, 4])].index
    print("noise index:\n", noise_index)

    labels = labels.drop(index=noise_index)
    labels = labels.set_index(0)

    for feature in subject_feats:

      subject_feats[feature] = subject_feats[feature].drop(index=noise_index*30)
    
    return subject_feats, labels

sub_feats, labels = remove_noise(sub_feats, labels)

# our hr feature data after noise index entries were removed for this subject...
sub_feats["hr"].loc[:510]

# ...and the matching labels
labels.loc[:510]

# now that the times all line up
# we can combine all the features in our dictionary to use as training/testing sets

print(labels.loc[:510], "\n")
print(sub_feats["motion"].loc[:510], "\n")
print(sub_feats["hr"].loc[:510], "\n")

# this function will loop through our feature set and horizontally join the dataframes together
# so our result is one dataframe with all feature columns 

def join_features(subject_features):

    # join the feature dfs horizontally
    joined_feats = pd.DataFrame([])

    for feature in subject_features:
        if feature == "motion":
          subject_features[feature] = subject_features[feature].rename(columns={1:"x",2:"y",3:"z"})
        elif feature == "hr":
          subject_features[feature] = subject_features[feature].rename(columns={1:"hr"})

        joined_feats = pd.concat([joined_feats, subject_features[feature]], axis=1)
    return joined_feats

joined_feats = join_features(sub_feats)
print(joined_feats)

# make training/testing dataset
# this returns a feature array of [time - motion x - motion y - motion z - hr] and an aligned array of labels  [time - label]
# for both the training and test set
# split paramter will create a set of "train" subjects and "test" subjects
# add_id paramter will include the subject id in the resulting dataframe - necessary for tsfresh feature extraction

from sklearn.model_selection import train_test_split

def create_train_test(FULL_SET, feature_set, split=True, test_size=0.2, add_id=False):

    training_set_features = pd.DataFrame([])
    training_set_true_labels = pd.DataFrame([])
    testing_set_features = pd.DataFrame([])
    testing_set_true_labels = pd.DataFrame([])

    if split == False:

        for subject in FULL_SET:

            subject_features, subject_labels = interpolate_features(subject, feature_set)

            subject_features, subject_labels = remove_noise(subject_features, subject_labels)

            joined_features = join_features(subject_features)

            if add_id == True:
                joined_features["id"] = [subject for i in range(0, len(subject_labels))]
                subject_labels["id"] = [subject for i in range(0, len(subject_labels))]

            if np.shape(training_set_features)[0] == 0:
                training_set_features = joined_features
                training_set_true_labels = subject_labels
            else:
                training_set_features = pd.concat((training_set_features, joined_features))
                training_set_true_labels = pd.concat((training_set_true_labels, subject_labels))

        return training_set_features, training_set_true_labels


    training_subjects, testing_subjects = train_test_split(FULL_SET, test_size = test_size)
    print("training set:\n", training_subjects, "\n")
    print("testing set:\n", testing_subjects, "\n")

    # Get labels and features for training and testing sets
    for subject in training_subjects:

        subject_features, subject_labels = interpolate_features(subject, feature_set)

        subject_features, subject_labels = remove_noise(subject_features, subject_labels)

        joined_features = join_features(subject_features)

        if np.shape(training_set_features)[0] == 0:
            training_set_features = joined_features
            training_set_true_labels = subject_labels
        else:
            training_set_features = pd.concat((training_set_features, joined_features))
            training_set_true_labels = pd.concat((training_set_true_labels, subject_labels))

    for subject in testing_subjects:

        subject_features, subject_labels = interpolate_features(subject, feature_set)

        subject_features, subject_labels = remove_noise(subject_features, subject_labels)

        joined_features = join_features(subject_features)

        if np.shape(testing_set_features)[0] == 0:
            testing_set_features = joined_features
            testing_set_true_labels = subject_labels
        else:
            testing_set_features = pd.concat((testing_set_features, joined_features))
            testing_set_true_labels = pd.concat((testing_set_true_labels, subject_labels))

    return training_set_features, training_set_true_labels, testing_set_features, testing_set_true_labels


training_set_features, training_set_true_labels, testing_set_features, testing_set_true_labels = create_train_test(FULL_SET, feature_set, test_size=0.2)

full_set_features, full_set_true_labels = create_train_test(FULL_SET, feature_set, split=False, add_id=True)

full_set_features.to_csv('/content/drive/My Drive/CA4015/sleep_classify/raw_windowed_features.csv')
full_set_true_labels.to_csv('/content/drive/My Drive/CA4015/sleep_classify/raw_windowed_labels.csv')


full_set_true_labels[1].unique()

print("Training Set Features", "\n", training_set_features)
print("Training Set Features", "\n", training_set_true_labels)

print("Testing Set Features", "\n", testing_set_features)
print("Testing Set Features", "\n", testing_set_true_labels)

!pip install tsfresh

# make sure our feature df is all numeric before extracting features
# it should be already but tsfresh was giving errors sometimes

full_set_features2 = full_set_features.reset_index()
full_set_features2 = full_set_features2.rename(columns={"index":"time"})
full_set_features2 = full_set_features2.apply(pd.to_numeric)
print(full_set_features2.head(), "\n")

# Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.

# This function is useful to massage a DataFrame into a format where one or more 
# columns are identifier variables (id_vars), while all other columns, considered 
# measured variables (value_vars), are “unpivoted” to the row axis, 
# leaving just two non-identifier columns, ‘variable’ and ‘value’.

full_set_features2['combine_id'] = list(zip(full_set_features2.id, full_set_features2.time))
full_set_features2 = full_set_features2.drop(columns="id")
melted_features = full_set_features2.melt(id_vars = ["combine_id", "time"])
print(melted_features, "\n")

melted_features.dtypes


full_set_true_labels = full_set_true_labels.apply(pd.to_numeric)
#full_set_true_labels2 = full_set_true_labels2.reset_index()
full_set_true_labels.head()

full_set_true_labels.dtypes

#new_labels = training_set_true_labels.reset_index(drop=True)
new_labels = pd.Series(full_set_true_labels.loc[:,1])
#new_labels = new_labels.reindex(np.arange(0, len(new_labels) + 1, 30))
new_labels.index = list(zip(full_set_true_labels["id"], full_set_true_labels.index))
print(new_labels.head(), "\n")
new_labels.dtypes

from tsfresh import extract_features


# this takes a while ~20mins
X = extract_features(melted_features, column_id="combine_id", column_value="value", column_sort="time")


print(X.shape)

# sort our labels on the index of time step, subject id

# our original labels
print(new_labels.head(), "\n")

# create multiIndex by separating id and time
a = pd.MultiIndex.from_tuples(new_labels.index, names=('id', 'time'))

# reindex our labels using this multiIndex
sorted_new_labels = new_labels.reindex(a)


# sort labels according to this new multiIndex
sorted_new_labels.sort_index(inplace=True)

sorted_new_labels.to_csv('/content/drive/My Drive/CA4015/sleep_classify/extracted_features_labels.csv')

sorted_new_labels

# Display Extracted Features for each time step of each subject
print(X)

X = X.dropna(axis='columns')
# Check shape of feature
X.shape

np.where(np.isinf(X))

X = X.drop(X.columns[[31]], axis=1) 

np.where(np.isinf(X))

# save to csv
X.to_csv('/content/drive/My Drive/CA4015/sleep_classify/extracted_features_2.csv')

import tsfresh
X = pd.read_csv('/content/drive/My Drive/CA4015/sleep_classify/extracted_features_2.csv')
Y = pd.read_csv('/content/drive/My Drive/CA4015/sleep_classify/extracted_features_labels.csv')
y_series = pd.Series(Y['1'])



tsfresh.feature_selection.relevance.calculate_relevance_table(X, y_series, ml_task='classification').head(10)
