
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Classification of windowed time series without feature extraction &#8212; CA4015 Assignment 2 - Time Series Classification - Jack Boylan, Daniel O&#39;Boyle, John Weldon</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/classification.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">CA4015 Assignment 2 - Time Series Classification - Jack Boylan, Daniel O'Boyle, John Weldon</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro_and_Desc_of_Data.html">
   INTRODUCTION AND DESCRIPTION OF DATASET
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Data Cleaning and Processing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="processing.html">
   Data Preprocessing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="processing.html#data-processing">
   Data Processing
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Classification Model
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html">
   Classification of windowed time series
   <strong>
    without
   </strong>
   feature extraction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#sklearn-binary-data-classifier">
   <strong>
    Sklearn - Binary Data Classifier
   </strong>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#sklearn-three-label-data-classifier">
   <strong>
    Sklearn - Three-Label Data Classifier
   </strong>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#sklearn-multi-label-data-classifier">
   <strong>
    Sklearn - Multi-Label Data Classifier
   </strong>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#extracted-features">
   <strong>
    Extracted features
   </strong>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#pca-of-extracted-features">
   PCA of Extracted Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#binary-data-classifier">
   Binary Data Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#three-label-classifier">
   Three-Label Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#multi-label-classifier">
   Multi-Label Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html#evaluation-of-model-performances">
   Evaluation of Model Performances
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Concluding Section
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Summary_Conclusion.html">
   Summary and Learning Outcomes
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/bckup_1classification.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/bckup_1classification.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Classification of windowed time series
   <strong>
    without
   </strong>
   feature extraction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automl">
     AutoML
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#mount-drive">
       Mount Drive
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#read-in-our-windowed-features-and-their-matching-labels-prepared-in-our-previous-data-processing">
     Read in our windowed features and their matching labels prepared in our previous data processing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sklearn-binary-data-classifier">
   <strong>
    Sklearn - Binary Data Classifier
   </strong>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#support-vector-machine-svm">
     Support Vector Machine (SVM)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree-classifier">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-our-models">
     Interpreting our models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-classification-algorithms">
     Comparing Classification Algorithms
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plotting-algorithm-performance">
     Plotting Algorithm Performance
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     AutoML
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nested-cross-validation">
     Nested Cross Validation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sklearn-three-label-data-classifier">
   <strong>
    Sklearn - Three-Label Data Classifier
   </strong>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Nested Cross Validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Comparing Classification Algorithms
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     Plotting Algorithm Performance
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     AutoML
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sklearn-multi-label-data-classifier">
   <strong>
    Sklearn - Multi-Label Data Classifier
   </strong>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     Nested Cross Validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     Comparing Classification Algorithms
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     Plotting Algorithm Performance
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     AutoML
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-of-extracted-features">
   PCA of Extracted Features
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     AutoML
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     Decision Tree Classifier
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     Nested Cross Validation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     Comparing Classification Algorithms
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     Plotting Algorithm Performance
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     AutoML
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="classification-of-windowed-time-series-without-feature-extraction">
<h1>Classification of windowed time series <strong>without</strong> feature extraction<a class="headerlink" href="#classification-of-windowed-time-series-without-feature-extraction" title="Permalink to this headline">¶</a></h1>
<p>Here we will make use of the processed data before using feature extraction methods to expand our data into higher dimensional space.</p>
<p>In the following cells we will use the original features to build and evaluate classifiers for</p>
<ul class="simple">
<li><p>binary (Sleep / Wake),</p></li>
<li><p>three (Sleep / NREM / REM),</p></li>
<li><p>and multi (Sleep / N1 / N2 / N3 / REM) label classification.</p></li>
</ul>
<div class="section" id="automl">
<h2>AutoML<a class="headerlink" href="#automl" title="Permalink to this headline">¶</a></h2>
<p>Automated machine learning (AutoML) involves automating the process of applying machine learning end-to-end. It makes ML accessible to non-experts as well as providing a quick and easy way to create model fitted on data without worrying about hyperparameters.</p>
<p>We will use AutoML later on in the notebook but we are installing it here because the runtime must be restarted afterwards.</p>
<p><em><strong>Note if you are running this book: run this cell, then restart the notebook’s runtime and then run all cells again after. This avoids version errors in auto-sklearn modules.</strong></em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!apt-get install swig -y
!pip install Cython numpy

# sometimes you have to run the next command twice on colab
# I haven&#39;t figured out why
!pip install auto-sklearn
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Reading package lists... Done
Building dependency tree       
Reading state information... Done
swig is already the newest version (3.0.12-1).
0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.
Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.21)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)
Requirement already satisfied: auto-sklearn in /usr/local/lib/python3.6/dist-packages (0.11.1)
Requirement already satisfied: ConfigSpace&lt;0.5,&gt;=0.4.14 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.4.16)
Requirement already satisfied: pandas&gt;=1.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.1.4)
Requirement already satisfied: scikit-learn&lt;0.23,&gt;=0.22.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.22.2.post1)
Requirement already satisfied: pynisher&gt;=0.6.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.6.1)
Requirement already satisfied: dask in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.12.0)
Requirement already satisfied: scipy&gt;=0.14.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.4.1)
Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (50.3.2)
Requirement already satisfied: pyrfr&lt;0.9,&gt;=0.7 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.8.0)
Requirement already satisfied: smac&lt;0.14,&gt;=0.13.1 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.13.1)
Requirement already satisfied: lockfile in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.12.2)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (3.13)
Requirement already satisfied: liac-arff in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.5.0)
Requirement already satisfied: numpy&gt;=1.9.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (1.18.5)
Requirement already satisfied: distributed&gt;=2.2.0 in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (2.30.1)
Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from auto-sklearn) (0.17.0)
Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from ConfigSpace&lt;0.5,&gt;=0.4.14-&gt;auto-sklearn) (2.4.7)
Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from ConfigSpace&lt;0.5,&gt;=0.4.14-&gt;auto-sklearn) (0.29.21)
Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=1.0-&gt;auto-sklearn) (2.8.1)
Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas&gt;=1.0-&gt;auto-sklearn) (2018.9)
Requirement already satisfied: docutils&gt;=0.3 in /usr/local/lib/python3.6/dist-packages (from pynisher&gt;=0.6.1-&gt;auto-sklearn) (0.16)
Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from pynisher&gt;=0.6.1-&gt;auto-sklearn) (5.4.8)
Requirement already satisfied: lazy-import in /usr/local/lib/python3.6/dist-packages (from smac&lt;0.14,&gt;=0.13.1-&gt;auto-sklearn) (0.2.2)
Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (2.2.2)
Requirement already satisfied: tblib&gt;=1.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (1.7.0)
Requirement already satisfied: click&gt;=6.6 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (7.1.2)
Requirement already satisfied: msgpack&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (1.0.0)
Requirement already satisfied: cloudpickle&gt;=1.5.0 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (1.6.0)
Requirement already satisfied: toolz&gt;=0.8.2 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (0.11.1)
Requirement already satisfied: tornado&gt;=5; python_version &lt; &quot;3.8&quot; in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (5.1.1)
Requirement already satisfied: zict&gt;=0.1.3 in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (2.0.0)
Requirement already satisfied: contextvars; python_version &lt; &quot;3.7&quot; in /usr/local/lib/python3.6/dist-packages (from distributed&gt;=2.2.0-&gt;auto-sklearn) (2.4)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=1.0-&gt;auto-sklearn) (1.15.0)
Requirement already satisfied: heapdict in /usr/local/lib/python3.6/dist-packages (from zict&gt;=0.1.3-&gt;distributed&gt;=2.2.0-&gt;auto-sklearn) (1.0.1)
Requirement already satisfied: immutables&gt;=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars; python_version &lt; &quot;3.7&quot;-&gt;distributed&gt;=2.2.0-&gt;auto-sklearn) (0.14)
</pre></div>
</div>
</div>
</div>
<div class="section" id="mount-drive">
<h3>Mount Drive<a class="headerlink" href="#mount-drive" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s1">&#39;/content/drive/My Drive/CA4015/sleep_classify&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True).
</pre></div>
</div>
</div>
</div>
<p>##<strong>Modelling the Windows as Independent Data Points</strong>##</p>
<p>This next part of the notebook involves looking at the interpolated data chunks as independent separate data points with the time element and patient ID removed. The model performance from this dataset will be a true indicator of how important the time series element is to classifying a patient’s sleep state, given only their motion and heart rate for that given window. It’s expected that this will result in an impaired model performance compared to the classification of the data with extracted features. Although an improvement would lead to the hypothesis that sleep classification only requires a static model of independent data points.</p>
</div>
</div>
<div class="section" id="read-in-our-windowed-features-and-their-matching-labels-prepared-in-our-previous-data-processing">
<h2>Read in our windowed features and their matching labels prepared in our previous data processing<a class="headerlink" href="#read-in-our-windowed-features-and-their-matching-labels-prepared-in-our-previous-data-processing" title="Permalink to this headline">¶</a></h2>
<p>In this approach we are using the original motion and heart rate features that were windowed at the end of our data processing operation. The time and ID columns are dropped from the dataframe as we are modelling purely based on the heart rate and motion features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># read in our windowed features and matching labels</span>

<span class="n">full_set_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/drive/My Drive/CA4015/sleep_classify/raw_windowed_features.csv&#39;</span><span class="p">)</span>
<span class="n">full_set_true_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/drive/My Drive/CA4015/sleep_classify/raw_windowed_labels.csv&#39;</span><span class="p">)</span>

<span class="c1"># we do not require id and time step columns for this approach</span>
<span class="n">full_set_features</span> <span class="o">=</span> <span class="n">full_set_features</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">full_set_true_labels</span> <span class="o">=</span> <span class="n">full_set_true_labels</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Ensure that the unique labels for our data lie in the range 0-5 with unknowns -1 and 4 removed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">full_set_true_labels</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 5])
</pre></div>
</div>
</div>
</div>
<p>Notice the new dataframe where the order and index of data points are arbitrary and provide no information about the given readings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Full Set Features&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">full_set_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Full Set Labels&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">full_set_true_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Full Set Features 
               x         y         z    hr
0     -0.612717 -0.086441 -0.774131  74.5
1     -0.608009 -0.078117 -0.779465  66.5
2     -0.608291 -0.074685 -0.776802  68.0
3     -0.604043 -0.074944 -0.782837  71.0
4     -0.607269 -0.070290 -0.782433  68.0
...         ...       ...       ...   ...
26412 -0.108566 -0.196350 -0.966415  87.0
26413 -0.106613 -0.198547 -0.966125  87.0
26414 -0.108292 -0.198059 -0.967621  87.0
26415 -0.107285 -0.199288 -0.968819  86.5
26416 -0.106293 -0.197815 -0.970017  87.0

[26417 rows x 4 columns]

Full Set Labels 
        1
0      0
1      0
2      0
3      0
4      0
...   ..
26412  2
26413  2
26414  2
26415  2
26416  2

[26417 rows x 1 columns]
</pre></div>
</div>
</div>
</div>
<p>Classification models require the target feature set to be in a numpy array. Here we unravel our labels from pandas Series format into a numpy array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">label_array</span> <span class="o">=</span> <span class="n">full_set_true_labels</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">label_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">label_array</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(26417,)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 0, 0, ..., 2, 2, 2])
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="sklearn-binary-data-classifier">
<h1><strong>Sklearn - Binary Data Classifier</strong><a class="headerlink" href="#sklearn-binary-data-classifier" title="Permalink to this headline">¶</a></h1>
<p>First, we will attempt to classify the data points as being either “Wake” or “Sleep”. The next cell will create a dataset which will replace any label greater than 0 with a 1, so 0 still represents “Wake”, while 1 represents “Sleep”.</p>
<p>Let’s now implement a simple classifier using our new <strong>binary</strong> dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># first we&#39;ll create a binary label set, indicating 0 for awake and 1 for asleep</span>

<span class="n">binary_label_array</span> <span class="o">=</span> <span class="n">label_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">binary_label_array</span><span class="p">[</span><span class="n">binary_label_array</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">binary_label_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing sets</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">full_set_features</span><span class="p">,</span> <span class="n">binary_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="support-vector-machine-svm">
<h2>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Permalink to this headline">¶</a></h2>
<p>SVMs are a type of supervised learning model that attempts to analyse input data given corresponding labels. SVMs are quite robust but unfortunately training complexity can reach <span class="math notranslate nohighlight">\(O(n2p+n3)\)</span> when using non-approximate or non-linear kernels, where <span class="math notranslate nohighlight">\(n\)</span> is the number of training examples and <span class="math notranslate nohighlight">\(p\)</span> is the number of features.</p>
<p>SVMs work by finding a separating hyperplane between data of different classes. SVM is an algorithm that takes the data as an input and outputs a line that separates those classes as best as it can.</p>
<p>A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Implementing a C-Support Vector Classifier, </span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">))</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Pipeline(memory=None,
         steps=[(&#39;standardscaler&#39;,
                 StandardScaler(copy=True, with_mean=True, with_std=True)),
                (&#39;svc&#39;,
                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,
                     coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3,
                     gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,
                     random_state=None, shrinking=True, tol=0.001,
                     verbose=False))],
         verbose=False)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9096139288417865
</pre></div>
</div>
</div>
</div>
<p>Not a bad score by any means, but as we will see later on, not the best score. Also, accuracy alone is not the best representation of model performance. We would expect the performance to be quite high for binary classification, as a human randomly guessing the label would be 50% accurate.</p>
</div>
<div class="section" id="decision-tree-classifier">
<h2>Decision Tree Classifier<a class="headerlink" href="#decision-tree-classifier" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the consistency of our classifier according to this Decision Tree Model. We can see below it is ~90% accurate with 95% confidence interval of 1%.</p>
<p>To train our model we use a technique called Cross Validation; by partitioning the available data into n sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.</p>
<p>A solution to this problem is a procedure called cross-validation. A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets.</p>
<p>The confusion matrix below shows how the decision tree performed on the test set. The top left entry indicates the number of examples correctly labelled “Wake” and the top right is the number of examples incorrectly labelled as “Sleep”. Then the bottom left and right indicate the number of examples were incorrectly labelled “Wake” and correctly labelled “Sleep” respectively.</p>
<p>The smaller the values off the diagonal of your confusion matrix are, the better it has performed on the given data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">export_graphviz</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># min_samples_leaf=200</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">full_set_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">importance</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">importance</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Test set accuracy:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.91 (+/- 0.01)

The list of features with importance: 
[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;hr&#39;]

importance
 [0.26051276 0.30081511 0.30410366 0.13456847]

Test set accuracy:
 0.9182437547312642 

Confusion Matrix
 [[ 303  304]
 [ 236 5762]] 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interpreting-our-models">
<h2>Interpreting our models<a class="headerlink" href="#interpreting-our-models" title="Permalink to this headline">¶</a></h2>
<p>The advantage of using more “basic” models like Decsion Trees is that they are much easier to interpret than more complex approaches such as neural networks. This is particularly important in real-world scenarios where a model is used to make high-stakes decisions; we want to know <strong>why</strong> a model has made a particular decision when given an input - such as a patient’s MRI scan - not just <strong>how</strong>. <a class="reference external" href="https://www.nature.com/articles/s42256-019-0048-x">A paper published in Nature (Rudin, C., 2019)</a> explores this topic further.</p>
<p>In our case, it’s not critical to know why a model has made a classification, but it is interesting to note that Decision Trees perform quite well on this data.</p>
<p>In the below cells we can project the Decision Tree on to an image, which would allow a human to follow the “thought process” of the model from start to classification given an input.</p>
<p>The resulting image from the above tree is far too large to see here, but if we set the parameter min_samples_leaf to a large value such as 200, it would force the tree to stop splitting earlier, making the image simpler and easier to read. However, doing so creates an issue which we will see later on, whereby the model simply guesses that all given inputs are classified as “Sleep”, because that is the majority label it is shown in training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install graphviz
!pip install imageio
import graphviz
import pydotplus
import io
from scipy import misc
import imageio
import matplotlib.pyplot as plt


def show_tree(tree, path):
    f = io.StringIO()
    export_graphviz(tree, out_file=f)
    pydotplus.graph_from_dot_data(f.getvalue()).write_png(path)
    img = imageio.imread(path)
    plt.rcParams[&quot;figure.figsize&quot;] = (30,20)
    plt.imshow(img)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)
Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)
Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">show_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="s2">&quot;binary_dec_tree.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.488491 to fit
</pre></div>
</div>
<img alt="_images/bckup_1classification_25_1.png" src="_images/bckup_1classification_25_1.png" />
</div>
</div>
</div>
<div class="section" id="comparing-classification-algorithms">
<h2>Comparing Classification Algorithms<a class="headerlink" href="#comparing-classification-algorithms" title="Permalink to this headline">¶</a></h2>
<p>When comparing the different algorithms for classifying our sleep stages, we must also look at how these algorithms choose a label. As shown below, all algorithms initially look very effective at classifying data, with all achieving accuracy of <span class="math notranslate nohighlight">\(&gt;= 90%\)</span>.</p>
<p>However, a closer inspection reveals that some algorithms are simply guessing every datapoint is in “Sleep” stage, because, for the majority of our points, this is true.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">model_selection</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="k">def</span> <span class="nf">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;LOGISTIC REGRESSION&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;LINEAR DISCRIMINANT AANALYSIS&quot;</span><span class="p">,</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;K NEAREST NEIGHBOURS&quot;</span><span class="p">,</span> <span class="n">KNeighborsClassifier</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;NAIVE BAYES&quot;</span><span class="p">,</span> <span class="n">GaussianNB</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;DECISION TREE&quot;</span><span class="p">,</span> <span class="n">DecisionTreeClassifier</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;RANDOM FOREST&quot;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;SUPPORT VECTOR MACHINE&quot;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">()))</span>
    <span class="n">models</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;ADABOOST CLASSIFIER&quot;</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span><span class="p">()))</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
      <span class="n">kfold</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">)</span>
      <span class="n">cv_results</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">kfold</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
      <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
      <span class="n">names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model  Mean Accuracy  95</span><span class="si">% c</span><span class="s2">onf interval&quot;</span><span class="p">)</span>
      <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">cv_results</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">cv_results</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

      <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set accuracy:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification report</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="p">,</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">names</span><span class="p">,</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.9080355200845979 0.011057478757985417

Test set accuracy:
 0.9080999242997729 

Confusion Matrix
 [[   0  607]
 [   0 5998]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       607
           1       0.91      1.00      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.45      0.50      0.48      6605
weighted avg       0.82      0.91      0.86      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.9080355200845979 0.011057478757985417

Test set accuracy:
 0.9080999242997729 

Confusion Matrix
 [[   0  607]
 [   0 5998]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       607
           1       0.91      1.00      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.45      0.50      0.48      6605
weighted avg       0.82      0.91      0.86      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.906672623016538 0.013060781303140236

Test set accuracy:
 0.9094625283875851 

Confusion Matrix
 [[ 107  500]
 [  98 5900]] 

Classification report
               precision    recall  f1-score   support

           0       0.52      0.18      0.26       607
           1       0.92      0.98      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.72      0.58      0.61      6605
weighted avg       0.89      0.91      0.89      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.9083888769750572 0.011097381149822732

Test set accuracy:
 0.9084027252081757 

Confusion Matrix
 [[   7  600]
 [   5 5993]] 

Classification report
               precision    recall  f1-score   support

           0       0.58      0.01      0.02       607
           1       0.91      1.00      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.75      0.51      0.49      6605
weighted avg       0.88      0.91      0.87      6605
 

Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.9141929816607928 0.012425338597474606

Test set accuracy:
 0.9162755488266465 

Confusion Matrix
 [[ 296  311]
 [ 242 5756]] 

Classification report
               precision    recall  f1-score   support

           0       0.55      0.49      0.52       607
           1       0.95      0.96      0.95      5998

    accuracy                           0.92      6605
   macro avg       0.75      0.72      0.74      6605
weighted avg       0.91      0.92      0.91      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.9385726969275728 0.005560705169040338

Test set accuracy:
 0.9398940196820591 

Confusion Matrix
 [[ 273  334]
 [  63 5935]] 

Classification report
               precision    recall  f1-score   support

           0       0.81      0.45      0.58       607
           1       0.95      0.99      0.97      5998

    accuracy                           0.94      6605
   macro avg       0.88      0.72      0.77      6605
weighted avg       0.93      0.94      0.93      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.9080355200845979 0.011057478757985417

Test set accuracy:
 0.9080999242997729 

Confusion Matrix
 [[   0  607]
 [   0 5998]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       607
           1       0.91      1.00      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.45      0.50      0.48      6605
weighted avg       0.82      0.91      0.86      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.9082878923944986 0.011041406441707589

Test set accuracy:
 0.9082513247539743 

Confusion Matrix
 [[   7  600]
 [   6 5992]] 

Classification report
               precision    recall  f1-score   support

           0       0.54      0.01      0.02       607
           1       0.91      1.00      0.95      5998

    accuracy                           0.91      6605
   macro avg       0.72      0.51      0.49      6605
weighted avg       0.87      0.91      0.87      6605
 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-algorithm-performance">
<h2>Plotting Algorithm Performance<a class="headerlink" href="#plotting-algorithm-performance" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison&quot;</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Binary Label Data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_31_0.png" src="_images/bckup_1classification_31_0.png" />
</div>
</div>
<p>The Decision Tree, and by extension the Random Forest algorithm, appear to be the best performing models for the binary classification.</p>
</div>
<div class="section" id="id1">
<h2>AutoML<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Automated machine learning (AutoML) involves automating the process of applying machine learning end-to-end. It makes ML accessible to non-experts as well as providing a quick and easy way to create model fitted on data without worrying about hyperparameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">(),</span> <span class="mi">9</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">sklearn</span>

<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:numexpr.utils:NumExpr defaulting to 2 threads.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.9356548069644209
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="nested-cross-validation">
<h2>Nested Cross Validation<a class="headerlink" href="#nested-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>We intended to include the nested cross validation through sklearn but it appears to be very computationally expensive when applied to this data, and ends up running for too long with no result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">mean</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">std</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="k">def</span> <span class="nf">nested_cross_val</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">KNeighborsClassifier</span><span class="p">()):</span>

    <span class="c1"># configure the cross-validation procedure</span>
    <span class="n">cv_inner</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># define search space</span>
    <span class="n">pgrid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;n_neighbors&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
              <span class="s2">&quot;weights&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;uniform&quot;</span><span class="p">,</span> <span class="s2">&quot;distance&quot;</span><span class="p">],</span>
              <span class="s2">&quot;algorithm&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;ball_tree&quot;</span><span class="p">,</span> <span class="s2">&quot;kd_tree&quot;</span><span class="p">],</span>
              <span class="s2">&quot;leaf_size&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span>
              <span class="s2">&quot;p&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
              <span class="p">}</span>

    <span class="c1"># define search</span>
    <span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pgrid</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_inner</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># configure the cross-validation procedure</span>
    <span class="n">cv_outer</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># execute the nested cross-validation</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">search</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_outer</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># report performance</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy: </span><span class="si">%.3f</span><span class="s1"> (</span><span class="si">%.3f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">std</span><span class="p">(</span><span class="n">scores</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="sklearn-three-label-data-classifier">
<h1><strong>Sklearn - Three-Label Data Classifier</strong><a class="headerlink" href="#sklearn-three-label-data-classifier" title="Permalink to this headline">¶</a></h1>
<p>Let’s now implement classifiers using a <strong>WAKE / NREM / REM label</strong> dataset.</p>
<p>First we’ll create a three label set, indicating 0 for awake, 1 for non-REM sleep and 2 for REM sleep</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">three_label_array</span> <span class="o">=</span> <span class="n">label_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">three_label_array</span><span class="p">[(</span><span class="n">three_label_array</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">three_label_array</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">three_label_array</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">three_label_array</span><span class="p">[</span><span class="n">three_label_array</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">three_label_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">full_set_features</span><span class="p">,</span> <span class="n">three_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id2">
<h2>Decision Tree Classifier<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the feature importance according to this Decision Tree Model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">full_set_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;importance</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">importance</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.83 (+/- 0.01)

The list of features with importance: 
[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;hr&#39;]
importance
 [0.29920924 0.29638919 0.26554737 0.13885419]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h2>Nested Cross Validation<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h2>Comparing Classification Algorithms<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.6854437030701859 0.015297374696694597

Test set accuracy:
 0.6850870552611658 

Confusion Matrix
 [[   0  601    0]
 [   0 4525    0]
 [   2 1477    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       601
           1       0.69      1.00      0.81      4525
           2       0.00      0.00      0.00      1479

    accuracy                           0.69      6605
   macro avg       0.23      0.33      0.27      6605
weighted avg       0.47      0.69      0.56      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.6854941826259658 0.015281281751026958

Test set accuracy:
 0.6852384557153671 

Confusion Matrix
 [[   1  600    0]
 [   0 4525    0]
 [   2 1477    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.33      0.00      0.00       601
           1       0.69      1.00      0.81      4525
           2       0.00      0.00      0.00      1479

    accuracy                           0.69      6605
   macro avg       0.34      0.33      0.27      6605
weighted avg       0.50      0.69      0.56      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.7061881517198451 0.01681505220940768

Test set accuracy:
 0.7200605601816805 

Confusion Matrix
 [[ 149  361   91]
 [ 182 4037  306]
 [ 104  805  570]] 

Classification report
               precision    recall  f1-score   support

           0       0.34      0.25      0.29       601
           1       0.78      0.89      0.83      4525
           2       0.59      0.39      0.47      1479

    accuracy                           0.72      6605
   macro avg       0.57      0.51      0.53      6605
weighted avg       0.69      0.72      0.70      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.6859987744317738 0.014689180863305543

Test set accuracy:
 0.6852384557153671 

Confusion Matrix
 [[  10  591    0]
 [   9 4516    0]
 [   4 1475    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.43      0.02      0.03       601
           1       0.69      1.00      0.81      4525
           2       0.00      0.00      0.00      1479

    accuracy                           0.69      6605
   macro avg       0.37      0.34      0.28      6605
weighted avg       0.51      0.69      0.56      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.839996770530942 0.014203204179065735

Test set accuracy:
 0.8457229371688115 

Confusion Matrix
 [[ 325  205   71]
 [ 177 4099  249]
 [  78  239 1162]] 

Classification report
               precision    recall  f1-score   support

           0       0.56      0.54      0.55       601
           1       0.90      0.91      0.90      4525
           2       0.78      0.79      0.78      1479

    accuracy                           0.85      6605
   macro avg       0.75      0.74      0.75      6605
weighted avg       0.84      0.85      0.85      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.8787100818013307 0.011076220791087055

Test set accuracy:
 0.8859954579863739 

Confusion Matrix
 [[ 305  234   62]
 [  72 4300  153]
 [  23  209 1247]] 

Classification report
               precision    recall  f1-score   support

           0       0.76      0.51      0.61       601
           1       0.91      0.95      0.93      4525
           2       0.85      0.84      0.85      1479

    accuracy                           0.89      6605
   macro avg       0.84      0.77      0.80      6605
weighted avg       0.88      0.89      0.88      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.6853932489834048 0.015416719489134893

Test set accuracy:
 0.6850870552611658 

Confusion Matrix
 [[   0  601    0]
 [   0 4525    0]
 [   0 1479    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       601
           1       0.69      1.00      0.81      4525
           2       0.00      0.00      0.00      1479

    accuracy                           0.69      6605
   macro avg       0.23      0.33      0.27      6605
weighted avg       0.47      0.69      0.56      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.690995588260014 0.015710790450869275

Test set accuracy:
 0.6912944738834217 

Confusion Matrix
 [[  18  570   13]
 [   9 4497   19]
 [   5 1423   51]] 

Classification report
               precision    recall  f1-score   support

           0       0.56      0.03      0.06       601
           1       0.69      0.99      0.82      4525
           2       0.61      0.03      0.07      1479

    accuracy                           0.69      6605
   macro avg       0.62      0.35      0.31      6605
weighted avg       0.66      0.69      0.58      6605
 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h2>Plotting Algorithm Performance<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Wake / NREM / REM Label Data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_48_0.png" src="_images/bckup_1classification_48_0.png" />
</div>
</div>
</div>
<div class="section" id="id6">
<h2>AutoML<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.8788796366389099
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="sklearn-multi-label-data-classifier">
<h1><strong>Sklearn - Multi-Label Data Classifier</strong><a class="headerlink" href="#sklearn-multi-label-data-classifier" title="Permalink to this headline">¶</a></h1>
<p>Let’s now implement a simple multiclass classifier using our <strong>original multi-label</strong> dataset</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">label_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>
<span class="c1"># Please don&#39;t ever train on your test data, that&#39;s a big no no!</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">full_set_features</span><span class="p">,</span> <span class="n">label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h2>Decision Tree Classifier<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the feature importance according to this Decision Tree Model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">full_set_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;importance</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">importance</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.73 (+/- 0.01)

The list of features with importance: 
[&#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;hr&#39;]
importance
 [0.28835684 0.31187007 0.24885919 0.15091391]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id8">
<h2>Nested Cross Validation<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id9">
<h2>Comparing Classification Algorithms<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.4886423801085081 0.01715752324924274

Test set accuracy:
 0.48962906888720664 

Confusion Matrix
 [[   1    0  569   10    3]
 [   0    0  453    2    0]
 [   0    0 3223   12   10]
 [   0    0  836    2    0]
 [   0    0 1476    0    8]] 

Classification report
               precision    recall  f1-score   support

           0       1.00      0.00      0.00       583
           1       0.00      0.00      0.00       455
           2       0.49      0.99      0.66      3245
           3       0.08      0.00      0.00       838
           5       0.38      0.01      0.01      1484

    accuracy                           0.49      6605
   macro avg       0.39      0.20      0.14      6605
weighted avg       0.43      0.49      0.33      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.4890966197035307 0.016960005628748605

Test set accuracy:
 0.48962906888720664 

Confusion Matrix
 [[   1    0  568   11    3]
 [   0    0  453    2    0]
 [   0    0 3223   12   10]
 [   0    0  836    2    0]
 [   0    0 1476    0    8]] 

Classification report
               precision    recall  f1-score   support

           0       1.00      0.00      0.00       583
           1       0.00      0.00      0.00       455
           2       0.49      0.99      0.66      3245
           3       0.07      0.00      0.00       838
           5       0.38      0.01      0.01      1484

    accuracy                           0.49      6605
   macro avg       0.39      0.20      0.14      6605
weighted avg       0.42      0.49      0.33      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.5772768648273635 0.03133879733818633

Test set accuracy:
 0.5887963663890992 

Confusion Matrix
 [[ 175   34  247   46   81]
 [  56   46  253   31   69]
 [ 134   91 2578  174  268]
 [  20    7  269  513   29]
 [ 104   38  643  122  577]] 

Classification report
               precision    recall  f1-score   support

           0       0.36      0.30      0.33       583
           1       0.21      0.10      0.14       455
           2       0.65      0.79      0.71      3245
           3       0.58      0.61      0.60       838
           5       0.56      0.39      0.46      1484

    accuracy                           0.59      6605
   macro avg       0.47      0.44      0.45      6605
weighted avg       0.56      0.59      0.57      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.4879863496353603 0.016979299637552656

Test set accuracy:
 0.48932626797880396 

Confusion Matrix
 [[  27    0  556    0    0]
 [   8    0  447    0    0]
 [  40    0 3205    0    0]
 [  13    0  825    0    0]
 [  23    0 1461    0    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.24      0.05      0.08       583
           1       0.00      0.00      0.00       455
           2       0.49      0.99      0.66      3245
           3       0.00      0.00      0.00       838
           5       0.00      0.00      0.00      1484

    accuracy                           0.49      6605
   macro avg       0.15      0.21      0.15      6605
weighted avg       0.26      0.49      0.33      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.7357655802780299 0.018385583234761665

Test set accuracy:
 0.7483724451173354 

Confusion Matrix
 [[ 308   82  109   19   65]
 [  71  125  195    8   56]
 [ 103  148 2704  140  150]
 [  18    6  125  674   15]
 [  68   65  196   23 1132]] 

Classification report
               precision    recall  f1-score   support

           0       0.54      0.53      0.54       583
           1       0.29      0.27      0.28       455
           2       0.81      0.83      0.82      3245
           3       0.78      0.80      0.79       838
           5       0.80      0.76      0.78      1484

    accuracy                           0.75      6605
   macro avg       0.65      0.64      0.64      6605
weighted avg       0.75      0.75      0.75      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.7943170004039383 0.016357184160300964

Test set accuracy:
 0.8078728236184709 

Confusion Matrix
 [[ 330   52  123   16   62]
 [  46  123  221    3   62]
 [  35   68 2925  111  106]
 [   7    0  109  708   14]
 [  33   22  170    9 1250]] 

Classification report
               precision    recall  f1-score   support

           0       0.73      0.57      0.64       583
           1       0.46      0.27      0.34       455
           2       0.82      0.90      0.86      3245
           3       0.84      0.84      0.84       838
           5       0.84      0.84      0.84      1484

    accuracy                           0.81      6605
   macro avg       0.74      0.68      0.70      6605
weighted avg       0.80      0.81      0.80      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.49005565485635233 0.01658854541959464

Test set accuracy:
 0.4912944738834216 

Confusion Matrix
 [[   0    0  583    0    0]
 [   0    0  455    0    0]
 [   0    0 3245    0    0]
 [   0    0  838    0    0]
 [   0    0 1484    0    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       583
           1       0.00      0.00      0.00       455
           2       0.49      1.00      0.66      3245
           3       0.00      0.00      0.00       838
           5       0.00      0.00      0.00      1484

    accuracy                           0.49      6605
   macro avg       0.10      0.20      0.13      6605
weighted avg       0.24      0.49      0.32      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.5090853776874251 0.021948714174575758

Test set accuracy:
 0.5064345193035579 

Confusion Matrix
 [[  40    1  519    6   17]
 [  14    0  429    1   11]
 [  16    0 3176   25   28]
 [   0    0  769   28   41]
 [  16    0 1344   23  101]] 

Classification report
               precision    recall  f1-score   support

           0       0.47      0.07      0.12       583
           1       0.00      0.00      0.00       455
           2       0.51      0.98      0.67      3245
           3       0.34      0.03      0.06       838
           5       0.51      0.07      0.12      1484

    accuracy                           0.51      6605
   macro avg       0.36      0.23      0.19      6605
weighted avg       0.45      0.51      0.37      6605
 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id10">
<h2>Plotting Algorithm Performance<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Wake/N1/N2/N3/REM Label Data&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_61_0.png" src="_images/bckup_1classification_61_0.png" />
</div>
</div>
</div>
<div class="section" id="id11">
<h2>AutoML<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.7943981831945496
</pre></div>
</div>
</div>
</div>
<p>#<strong>Extracted features</strong></p>
<p>We will now be performing classification on the extracted feature dataset and observing the model results. This is time series classification as the time and ID columns are now included and the many extra features that TSFresh has provided have encoded information in each row about the time series. We can now treat it like any normal non-time series dataset while still being sure the time series information is retained for our analysis and classification.</p>
<p>Again, we will perform classification with the three different label sets of Binary: Wake/Sleep, Three classes: Wake, Non-REM Sleep and REM Sleep and finally with all original labels of sleep classes: (0, 1, 2, 3, 5).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">extracted_features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/drive/My Drive/CA4015/sleep_classify/extracted_features.csv&#39;</span><span class="p">)</span>
<span class="n">extracted_labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/drive/My Drive/CA4015/sleep_classify/extracted_features_labels.csv&#39;</span><span class="p">)</span>

<span class="n">extracted_features</span> <span class="o">=</span> <span class="n">extracted_features</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">:</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;Unnamed: 1&#39;</span><span class="p">:</span> <span class="s1">&#39;time&#39;</span><span class="p">})</span>

<span class="n">extracted_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>time</th>
      <th>value__variance_larger_than_standard_deviation</th>
      <th>value__has_duplicate_max</th>
      <th>value__has_duplicate_min</th>
      <th>value__has_duplicate</th>
      <th>value__sum_values</th>
      <th>value__abs_energy</th>
      <th>value__mean_abs_change</th>
      <th>value__mean_change</th>
      <th>value__mean_second_derivative_central</th>
      <th>value__median</th>
      <th>value__mean</th>
      <th>value__length</th>
      <th>value__standard_deviation</th>
      <th>value__variation_coefficient</th>
      <th>value__variance</th>
      <th>value__skewness</th>
      <th>value__kurtosis</th>
      <th>value__absolute_sum_of_changes</th>
      <th>value__longest_strike_below_mean</th>
      <th>value__longest_strike_above_mean</th>
      <th>value__count_above_mean</th>
      <th>value__count_below_mean</th>
      <th>value__last_location_of_maximum</th>
      <th>value__first_location_of_maximum</th>
      <th>value__last_location_of_minimum</th>
      <th>value__first_location_of_minimum</th>
      <th>value__percentage_of_reoccurring_values_to_all_values</th>
      <th>value__percentage_of_reoccurring_datapoints_to_all_datapoints</th>
      <th>value__sum_of_reoccurring_values</th>
      <th>value__sum_of_reoccurring_data_points</th>
      <th>value__ratio_value_number_to_time_series_length</th>
      <th>value__maximum</th>
      <th>value__minimum</th>
      <th>value__benford_correlation</th>
      <th>value__time_reversal_asymmetry_statistic__lag_1</th>
      <th>value__time_reversal_asymmetry_statistic__lag_2</th>
      <th>value__time_reversal_asymmetry_statistic__lag_3</th>
      <th>value__c3__lag_1</th>
      <th>...</th>
      <th>value__augmented_dickey_fuller__attr_"teststat"__autolag_"AIC"</th>
      <th>value__augmented_dickey_fuller__attr_"pvalue"__autolag_"AIC"</th>
      <th>value__augmented_dickey_fuller__attr_"usedlag"__autolag_"AIC"</th>
      <th>value__number_crossing_m__m_0</th>
      <th>value__number_crossing_m__m_-1</th>
      <th>value__number_crossing_m__m_1</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_0</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_1</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_2</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_3</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_4</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_5</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_6</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_7</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_8</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_9</th>
      <th>value__ratio_beyond_r_sigma__r_0.5</th>
      <th>value__ratio_beyond_r_sigma__r_1</th>
      <th>value__ratio_beyond_r_sigma__r_1.5</th>
      <th>value__ratio_beyond_r_sigma__r_2</th>
      <th>value__ratio_beyond_r_sigma__r_2.5</th>
      <th>value__ratio_beyond_r_sigma__r_3</th>
      <th>value__ratio_beyond_r_sigma__r_5</th>
      <th>value__ratio_beyond_r_sigma__r_6</th>
      <th>value__ratio_beyond_r_sigma__r_7</th>
      <th>value__ratio_beyond_r_sigma__r_10</th>
      <th>value__count_above__t_0</th>
      <th>value__count_below__t_0</th>
      <th>value__lempel_ziv_complexity__bins_2</th>
      <th>value__lempel_ziv_complexity__bins_3</th>
      <th>value__lempel_ziv_complexity__bins_5</th>
      <th>value__lempel_ziv_complexity__bins_10</th>
      <th>value__lempel_ziv_complexity__bins_100</th>
      <th>value__fourier_entropy__bins_2</th>
      <th>value__fourier_entropy__bins_3</th>
      <th>value__fourier_entropy__bins_5</th>
      <th>value__fourier_entropy__bins_10</th>
      <th>value__fourier_entropy__bins_100</th>
      <th>value__permutation_entropy__dimension_3__tau_1</th>
      <th>value__permutation_entropy__dimension_4__tau_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>46343</td>
      <td>390.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>89.910614</td>
      <td>8100.838344</td>
      <td>30.793696</td>
      <td>30.186508</td>
      <td>22.242584</td>
      <td>0.235069</td>
      <td>22.477653</td>
      <td>4.0</td>
      <td>38.986724</td>
      <td>1.734466</td>
      <td>1519.964679</td>
      <td>1.999177</td>
      <td>3.997235</td>
      <td>92.381088</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>90.0</td>
      <td>-0.559524</td>
      <td>-0.220047</td>
      <td>-892.340880</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-6.802985</td>
      <td>...</td>
      <td>-0.286355</td>
      <td>0.927415</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.000039</td>
      <td>0.000059</td>
      <td>0.000006</td>
      <td>0.999897</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.636514</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>0.693147</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>46343</td>
      <td>420.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>100.727615</td>
      <td>10201.949516</td>
      <td>34.534958</td>
      <td>33.877009</td>
      <td>25.002583</td>
      <td>0.179321</td>
      <td>25.181904</td>
      <td>4.0</td>
      <td>43.776239</td>
      <td>1.738401</td>
      <td>1916.359096</td>
      <td>1.999278</td>
      <td>3.997573</td>
      <td>103.604874</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>101.0</td>
      <td>-0.631027</td>
      <td>0.240954</td>
      <td>-1602.303277</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-10.606410</td>
      <td>...</td>
      <td>-0.318167</td>
      <td>0.922892</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.000039</td>
      <td>0.000044</td>
      <td>0.000010</td>
      <td>0.999907</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.636514</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>0.693147</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>46343</td>
      <td>450.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>93.513177</td>
      <td>8465.342833</td>
      <td>30.757888</td>
      <td>30.705180</td>
      <td>22.563936</td>
      <td>0.814358</td>
      <td>23.378294</td>
      <td>4.0</td>
      <td>39.620589</td>
      <td>1.694760</td>
      <td>1569.791072</td>
      <td>1.999445</td>
      <td>3.998166</td>
      <td>92.273666</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>92.0</td>
      <td>-0.115540</td>
      <td>0.033057</td>
      <td>3279.033894</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>30.396124</td>
      <td>...</td>
      <td>0.470899</td>
      <td>0.983961</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.000002</td>
      <td>0.000086</td>
      <td>0.000071</td>
      <td>0.999841</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.636514</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>0.693147</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>46343</td>
      <td>480.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>82.258003</td>
      <td>6724.989743</td>
      <td>27.448583</td>
      <td>27.448583</td>
      <td>20.262625</td>
      <td>0.301876</td>
      <td>20.564501</td>
      <td>4.0</td>
      <td>35.473212</td>
      <td>1.724973</td>
      <td>1258.348745</td>
      <td>1.998850</td>
      <td>3.996119</td>
      <td>82.345749</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>82.0</td>
      <td>-0.345749</td>
      <td>-0.085823</td>
      <td>2986.556895</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-10.322812</td>
      <td>...</td>
      <td>31.239292</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.000018</td>
      <td>0.000012</td>
      <td>0.000117</td>
      <td>0.999853</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.636514</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>-0.000000</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>46343</td>
      <td>510.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>83.314782</td>
      <td>6889.998766</td>
      <td>27.771871</td>
      <td>27.771871</td>
      <td>20.513498</td>
      <td>0.315197</td>
      <td>20.828695</td>
      <td>4.0</td>
      <td>35.897982</td>
      <td>1.723487</td>
      <td>1288.665135</td>
      <td>1.998880</td>
      <td>3.996217</td>
      <td>83.315613</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.75</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>83.0</td>
      <td>-0.315613</td>
      <td>-0.133823</td>
      <td>3124.019917</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-10.371881</td>
      <td>...</td>
      <td>62.857719</td>
      <td>1.000000</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.000014</td>
      <td>0.000011</td>
      <td>0.000119</td>
      <td>0.999855</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.25</td>
      <td>0.25</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>0.75</td>
      <td>1.0</td>
      <td>0.636514</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>1.098612</td>
      <td>-0.000000</td>
      <td>-0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 271 columns</p>
</div></div></div>
</div>
</div>
</div>
<div class="section" id="pca-of-extracted-features">
<h1>PCA of Extracted Features<a class="headerlink" href="#pca-of-extracted-features" title="Permalink to this headline">¶</a></h1>
<p>Principal Component Analysis is often used in ML when the number of dimensions is very high. The much smaller number of Principal Components capture the majority of the variance in the original dataset, and thus can be used to train the model instead. The smaller number of dimensions with PCA means faster training times and simplified data structure when feeding into ML models.</p>
<p>Conversely, the Principal Components created from reducing the original dimensions have no interpretability to the user, as we cannot trace back what makes up each PC. Therefore, any model created using PCA will not be very transparent, making it a poor choice for solving real-world high-stakes problems.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">feats</span> <span class="o">=</span> <span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
<span class="n">feats</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;value__variance_larger_than_standard_deviation&#39;,
       &#39;value__has_duplicate_max&#39;, &#39;value__has_duplicate_min&#39;,
       &#39;value__has_duplicate&#39;, &#39;value__sum_values&#39;, &#39;value__abs_energy&#39;,
       &#39;value__mean_abs_change&#39;, &#39;value__mean_change&#39;,
       &#39;value__mean_second_derivative_central&#39;, &#39;value__median&#39;,
       ...
       &#39;value__lempel_ziv_complexity__bins_5&#39;,
       &#39;value__lempel_ziv_complexity__bins_10&#39;,
       &#39;value__lempel_ziv_complexity__bins_100&#39;,
       &#39;value__fourier_entropy__bins_2&#39;, &#39;value__fourier_entropy__bins_3&#39;,
       &#39;value__fourier_entropy__bins_5&#39;, &#39;value__fourier_entropy__bins_10&#39;,
       &#39;value__fourier_entropy__bins_100&#39;,
       &#39;value__permutation_entropy__dimension_3__tau_1&#39;,
       &#39;value__permutation_entropy__dimension_4__tau_1&#39;],
      dtype=&#39;object&#39;, length=269)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="n">standardised</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">extracted_features</span><span class="p">[</span><span class="n">feats</span><span class="p">])</span>

<span class="n">details</span> <span class="o">=</span> <span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">df1_std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">standardised</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">feats</span><span class="p">)</span>
<span class="n">df1_std</span> <span class="o">=</span> <span class="n">df1_std</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="c1">#df1_std = pd.concat([extracted_features[details], df1_std], axis=1)</span>
<span class="n">df1_std</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/scipy/stats/stats.py:2419: RuntimeWarning: invalid value encountered in true_divide
  return (a - mns) / sstd
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>value__sum_values</th>
      <th>value__abs_energy</th>
      <th>value__mean_abs_change</th>
      <th>value__mean_change</th>
      <th>value__mean_second_derivative_central</th>
      <th>value__median</th>
      <th>value__mean</th>
      <th>value__standard_deviation</th>
      <th>value__variation_coefficient</th>
      <th>value__variance</th>
      <th>value__skewness</th>
      <th>value__kurtosis</th>
      <th>value__absolute_sum_of_changes</th>
      <th>value__last_location_of_minimum</th>
      <th>value__first_location_of_minimum</th>
      <th>value__maximum</th>
      <th>value__minimum</th>
      <th>value__benford_correlation</th>
      <th>value__time_reversal_asymmetry_statistic__lag_1</th>
      <th>value__c3__lag_1</th>
      <th>value__cid_ce__normalize_True</th>
      <th>value__cid_ce__normalize_False</th>
      <th>value__quantile__q_0.1</th>
      <th>value__quantile__q_0.2</th>
      <th>value__quantile__q_0.3</th>
      <th>value__quantile__q_0.4</th>
      <th>value__quantile__q_0.6</th>
      <th>value__quantile__q_0.7</th>
      <th>value__quantile__q_0.8</th>
      <th>value__quantile__q_0.9</th>
      <th>value__autocorrelation__lag_1</th>
      <th>value__autocorrelation__lag_2</th>
      <th>value__autocorrelation__lag_3</th>
      <th>value__agg_autocorrelation__f_agg_"mean"__maxlag_40</th>
      <th>value__agg_autocorrelation__f_agg_"median"__maxlag_40</th>
      <th>value__agg_autocorrelation__f_agg_"var"__maxlag_40</th>
      <th>value__partial_autocorrelation__lag_1</th>
      <th>value__number_cwt_peaks__n_1</th>
      <th>value__number_peaks__n_1</th>
      <th>value__binned_entropy__max_bins_10</th>
      <th>...</th>
      <th>value__fft_coefficient__attr_"abs"__coeff_0</th>
      <th>value__fft_coefficient__attr_"abs"__coeff_1</th>
      <th>value__fft_coefficient__attr_"abs"__coeff_2</th>
      <th>value__fft_coefficient__attr_"angle"__coeff_1</th>
      <th>value__fft_aggregated__aggtype_"centroid"</th>
      <th>value__fft_aggregated__aggtype_"variance"</th>
      <th>value__fft_aggregated__aggtype_"skew"</th>
      <th>value__fft_aggregated__aggtype_"kurtosis"</th>
      <th>value__value_count__value_0</th>
      <th>value__value_count__value_1</th>
      <th>value__range_count__max_1__min_-1</th>
      <th>value__range_count__max_1000000000000.0__min_0</th>
      <th>value__friedrich_coefficients__coeff_0__m_3__r_30</th>
      <th>value__friedrich_coefficients__coeff_1__m_3__r_30</th>
      <th>value__friedrich_coefficients__coeff_2__m_3__r_30</th>
      <th>value__friedrich_coefficients__coeff_3__m_3__r_30</th>
      <th>value__max_langevin_fixed_point__m_3__r_30</th>
      <th>value__linear_trend__attr_"pvalue"</th>
      <th>value__linear_trend__attr_"rvalue"</th>
      <th>value__linear_trend__attr_"intercept"</th>
      <th>value__linear_trend__attr_"slope"</th>
      <th>value__linear_trend__attr_"stderr"</th>
      <th>value__augmented_dickey_fuller__attr_"teststat"__autolag_"AIC"</th>
      <th>value__augmented_dickey_fuller__attr_"pvalue"__autolag_"AIC"</th>
      <th>value__number_crossing_m__m_0</th>
      <th>value__number_crossing_m__m_-1</th>
      <th>value__number_crossing_m__m_1</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_0</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_1</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_2</th>
      <th>value__energy_ratio_by_chunks__num_segments_10__segment_focus_3</th>
      <th>value__count_above__t_0</th>
      <th>value__count_below__t_0</th>
      <th>value__lempel_ziv_complexity__bins_2</th>
      <th>value__lempel_ziv_complexity__bins_100</th>
      <th>value__fourier_entropy__bins_3</th>
      <th>value__fourier_entropy__bins_5</th>
      <th>value__fourier_entropy__bins_10</th>
      <th>value__fourier_entropy__bins_100</th>
      <th>value__permutation_entropy__dimension_3__tau_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.575756</td>
      <td>2.821527</td>
      <td>2.563929</td>
      <td>2.574612</td>
      <td>2.389934</td>
      <td>0.678840</td>
      <td>2.575756</td>
      <td>2.515752</td>
      <td>-0.835504</td>
      <td>2.795317</td>
      <td>0.709471</td>
      <td>0.700269</td>
      <td>2.563929</td>
      <td>-2.485073</td>
      <td>-2.485073</td>
      <td>2.533013</td>
      <td>0.825217</td>
      <td>-0.621031</td>
      <td>0.082009</td>
      <td>-0.582327</td>
      <td>-0.203491</td>
      <td>2.501582</td>
      <td>0.665751</td>
      <td>0.371718</td>
      <td>0.079868</td>
      <td>0.291610</td>
      <td>0.936568</td>
      <td>2.677247</td>
      <td>2.577357</td>
      <td>2.546539</td>
      <td>-0.025365</td>
      <td>0.813961</td>
      <td>-1.494746</td>
      <td>-1.054467</td>
      <td>0.813961</td>
      <td>1.395581</td>
      <td>-0.025365</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>2.575756</td>
      <td>2.463885</td>
      <td>2.602619</td>
      <td>0.800593</td>
      <td>-0.076038</td>
      <td>1.260173</td>
      <td>0.074574</td>
      <td>-0.158207</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.017373</td>
      <td>-0.006344</td>
      <td>-0.012000</td>
      <td>0.005699</td>
      <td>0.007450</td>
      <td>-0.798195</td>
      <td>0.798195</td>
      <td>-2.510599</td>
      <td>2.560623</td>
      <td>2.438505</td>
      <td>0.029030</td>
      <td>1.061956</td>
      <td>1.294470</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>0.099459</td>
      <td>0.020001</td>
      <td>-1.649423</td>
      <td>1.898409</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>1.509692</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.639897</td>
      <td>4.361842</td>
      <td>3.667110</td>
      <td>3.670183</td>
      <td>3.483406</td>
      <td>0.486899</td>
      <td>3.639897</td>
      <td>3.611803</td>
      <td>-0.697872</td>
      <td>4.340198</td>
      <td>0.850319</td>
      <td>0.842304</td>
      <td>3.667110</td>
      <td>-2.485073</td>
      <td>-2.485073</td>
      <td>3.621965</td>
      <td>0.531841</td>
      <td>1.075842</td>
      <td>-0.403956</td>
      <td>-1.029758</td>
      <td>-0.193166</td>
      <td>3.596822</td>
      <td>0.296412</td>
      <td>-0.018904</td>
      <td>-0.274987</td>
      <td>0.002243</td>
      <td>0.838773</td>
      <td>3.673564</td>
      <td>3.657761</td>
      <td>3.633384</td>
      <td>-0.019141</td>
      <td>0.751283</td>
      <td>-1.388319</td>
      <td>-0.981523</td>
      <td>0.751283</td>
      <td>1.291465</td>
      <td>-0.019141</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>3.639897</td>
      <td>3.558150</td>
      <td>3.695714</td>
      <td>0.745423</td>
      <td>-0.018201</td>
      <td>1.122919</td>
      <td>0.016814</td>
      <td>-0.092655</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.019256</td>
      <td>-0.005550</td>
      <td>-0.017058</td>
      <td>0.007229</td>
      <td>-0.012335</td>
      <td>-0.744714</td>
      <td>0.744714</td>
      <td>-3.629856</td>
      <td>3.655089</td>
      <td>3.532175</td>
      <td>0.028730</td>
      <td>1.050651</td>
      <td>1.294470</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>0.110137</td>
      <td>-0.172726</td>
      <td>-1.611253</td>
      <td>2.026839</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>1.509692</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.930164</td>
      <td>3.088744</td>
      <td>2.553370</td>
      <td>2.728586</td>
      <td>2.517249</td>
      <td>2.673343</td>
      <td>2.930164</td>
      <td>2.660808</td>
      <td>-2.224376</td>
      <td>2.989507</td>
      <td>1.084491</td>
      <td>1.091760</td>
      <td>2.553370</td>
      <td>-2.485073</td>
      <td>-2.485073</td>
      <td>2.731005</td>
      <td>2.646890</td>
      <td>0.310606</td>
      <td>2.937288</td>
      <td>3.793738</td>
      <td>-0.829176</td>
      <td>2.599990</td>
      <td>3.543734</td>
      <td>3.936322</td>
      <td>3.722027</td>
      <td>3.343221</td>
      <td>1.916661</td>
      <td>2.996232</td>
      <td>2.798404</td>
      <td>2.751209</td>
      <td>0.613299</td>
      <td>0.225659</td>
      <td>-1.674212</td>
      <td>-1.491088</td>
      <td>0.225659</td>
      <td>1.654504</td>
      <td>0.613299</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>2.930164</td>
      <td>2.646662</td>
      <td>2.673641</td>
      <td>1.315625</td>
      <td>-1.390718</td>
      <td>1.476636</td>
      <td>1.393559</td>
      <td>-1.462467</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>1.408142</td>
      <td>-0.013931</td>
      <td>0.005082</td>
      <td>0.045102</td>
      <td>0.000303</td>
      <td>-0.026612</td>
      <td>-1.315240</td>
      <td>1.315240</td>
      <td>-2.473228</td>
      <td>2.742265</td>
      <td>2.527666</td>
      <td>0.036175</td>
      <td>1.203283</td>
      <td>-1.003881</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.928831</td>
      <td>0.383045</td>
      <td>-0.976153</td>
      <td>1.218585</td>
      <td>1.408142</td>
      <td>-1.408143</td>
      <td>1.509692</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.822917</td>
      <td>1.812899</td>
      <td>1.577559</td>
      <td>1.761825</td>
      <td>1.605502</td>
      <td>0.908857</td>
      <td>1.822917</td>
      <td>1.711706</td>
      <td>-1.167549</td>
      <td>1.775712</td>
      <td>0.252683</td>
      <td>0.230670</td>
      <td>1.577559</td>
      <td>-2.485073</td>
      <td>-2.485073</td>
      <td>1.741048</td>
      <td>1.702339</td>
      <td>-0.126972</td>
      <td>2.737089</td>
      <td>-0.996396</td>
      <td>-1.662039</td>
      <td>1.601851</td>
      <td>1.281438</td>
      <td>0.587681</td>
      <td>-0.061497</td>
      <td>0.247967</td>
      <td>1.368898</td>
      <td>2.105714</td>
      <td>1.818937</td>
      <td>1.763948</td>
      <td>1.554594</td>
      <td>-0.940718</td>
      <td>-1.370131</td>
      <td>-1.728830</td>
      <td>-0.940718</td>
      <td>1.516554</td>
      <td>1.554594</td>
      <td>-1.281185</td>
      <td>-0.986427</td>
      <td>-1.0</td>
      <td>...</td>
      <td>1.822917</td>
      <td>1.767022</td>
      <td>1.592882</td>
      <td>1.766695</td>
      <td>-1.347476</td>
      <td>-0.244314</td>
      <td>1.347835</td>
      <td>-1.352174</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.013375</td>
      <td>0.011373</td>
      <td>-0.006629</td>
      <td>-0.010052</td>
      <td>-0.077188</td>
      <td>-1.768622</td>
      <td>1.768622</td>
      <td>-1.784438</td>
      <td>1.815611</td>
      <td>1.548347</td>
      <td>0.326489</td>
      <td>1.243368</td>
      <td>-1.003881</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.479480</td>
      <td>-0.602860</td>
      <td>-0.494656</td>
      <td>1.359797</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>1.509692</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>-4.101858</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.926879</td>
      <td>1.933866</td>
      <td>1.672887</td>
      <td>1.857797</td>
      <td>1.704894</td>
      <td>0.954722</td>
      <td>1.926879</td>
      <td>1.808912</td>
      <td>-1.219538</td>
      <td>1.893866</td>
      <td>0.293429</td>
      <td>0.272173</td>
      <td>1.672887</td>
      <td>-2.485073</td>
      <td>-2.485073</td>
      <td>1.840044</td>
      <td>1.825989</td>
      <td>-0.303655</td>
      <td>2.831182</td>
      <td>-1.002169</td>
      <td>-1.647296</td>
      <td>1.698741</td>
      <td>1.392427</td>
      <td>0.665265</td>
      <td>-0.021886</td>
      <td>0.285284</td>
      <td>1.417959</td>
      <td>2.213028</td>
      <td>1.920145</td>
      <td>1.863610</td>
      <td>1.545324</td>
      <td>-0.951348</td>
      <td>-1.331114</td>
      <td>-1.696508</td>
      <td>-0.951348</td>
      <td>1.477581</td>
      <td>1.545324</td>
      <td>-1.281185</td>
      <td>-0.986427</td>
      <td>-1.0</td>
      <td>...</td>
      <td>1.926879</td>
      <td>1.865519</td>
      <td>1.686942</td>
      <td>1.739371</td>
      <td>-1.389789</td>
      <td>-0.226229</td>
      <td>1.390148</td>
      <td>-1.394316</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.013342</td>
      <td>0.010769</td>
      <td>-0.006117</td>
      <td>-0.009683</td>
      <td>-0.071882</td>
      <td>-1.741605</td>
      <td>1.741605</td>
      <td>-1.870283</td>
      <td>1.912057</td>
      <td>1.646311</td>
      <td>0.624825</td>
      <td>1.243368</td>
      <td>-1.003881</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.571533</td>
      <td>-0.615382</td>
      <td>-0.473456</td>
      <td>1.387107</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>1.509692</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>-4.101858</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>26412</th>
      <td>0.586479</td>
      <td>0.579097</td>
      <td>0.679011</td>
      <td>0.677679</td>
      <td>0.662629</td>
      <td>-0.813086</td>
      <td>0.586479</td>
      <td>0.673100</td>
      <td>0.602314</td>
      <td>0.601088</td>
      <td>0.950920</td>
      <td>0.953463</td>
      <td>0.679011</td>
      <td>0.610225</td>
      <td>0.610225</td>
      <td>0.652096</td>
      <td>-0.628797</td>
      <td>-1.387289</td>
      <td>-0.883915</td>
      <td>0.243770</td>
      <td>0.347226</td>
      <td>0.693624</td>
      <td>-0.745900</td>
      <td>-0.748161</td>
      <td>-0.648075</td>
      <td>-0.720763</td>
      <td>-0.821923</td>
      <td>0.354357</td>
      <td>0.603721</td>
      <td>0.638423</td>
      <td>-0.403789</td>
      <td>0.534872</td>
      <td>-0.196037</td>
      <td>0.055173</td>
      <td>0.534872</td>
      <td>0.095589</td>
      <td>-0.403789</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>0.586479</td>
      <td>0.646179</td>
      <td>0.722540</td>
      <td>-0.163899</td>
      <td>0.733267</td>
      <td>0.177878</td>
      <td>-0.733328</td>
      <td>0.733725</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>-1.389292</td>
      <td>0.009516</td>
      <td>0.011951</td>
      <td>-0.006709</td>
      <td>-0.011384</td>
      <td>-0.026588</td>
      <td>0.157327</td>
      <td>-0.157327</td>
      <td>-0.747339</td>
      <td>0.661371</td>
      <td>0.688206</td>
      <td>0.010514</td>
      <td>-0.783488</td>
      <td>-1.003881</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.137042</td>
      <td>-0.763034</td>
      <td>0.006194</td>
      <td>0.760487</td>
      <td>-1.389292</td>
      <td>1.389189</td>
      <td>-0.662387</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>26413</th>
      <td>0.870085</td>
      <td>0.843931</td>
      <td>0.966154</td>
      <td>0.912148</td>
      <td>0.898182</td>
      <td>-0.133647</td>
      <td>0.870085</td>
      <td>0.908430</td>
      <td>0.117762</td>
      <td>0.853170</td>
      <td>0.414849</td>
      <td>0.417961</td>
      <td>0.966154</td>
      <td>0.610225</td>
      <td>0.610225</td>
      <td>0.899585</td>
      <td>-0.677286</td>
      <td>0.277202</td>
      <td>-1.018665</td>
      <td>-0.808329</td>
      <td>0.554122</td>
      <td>0.942044</td>
      <td>-0.600017</td>
      <td>-0.409538</td>
      <td>-0.197483</td>
      <td>-0.141762</td>
      <td>-0.116300</td>
      <td>0.802598</td>
      <td>0.888871</td>
      <td>0.896796</td>
      <td>-0.624013</td>
      <td>0.766732</td>
      <td>-0.189245</td>
      <td>0.166413</td>
      <td>0.766732</td>
      <td>0.063520</td>
      <td>-0.624013</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>0.870085</td>
      <td>0.868354</td>
      <td>0.981941</td>
      <td>-0.312110</td>
      <td>0.545136</td>
      <td>0.672281</td>
      <td>-0.546684</td>
      <td>0.508510</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.010523</td>
      <td>0.012186</td>
      <td>-0.009079</td>
      <td>-0.011829</td>
      <td>-0.029456</td>
      <td>0.307360</td>
      <td>-0.307360</td>
      <td>-0.897537</td>
      <td>0.887027</td>
      <td>0.936782</td>
      <td>0.011203</td>
      <td>-0.718133</td>
      <td>1.294470</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.627735</td>
      <td>-0.600044</td>
      <td>-0.066929</td>
      <td>0.914520</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>-0.662387</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>26414</th>
      <td>0.230215</td>
      <td>0.174431</td>
      <td>0.327314</td>
      <td>0.269362</td>
      <td>0.253973</td>
      <td>-0.142565</td>
      <td>0.230215</td>
      <td>0.264530</td>
      <td>0.225697</td>
      <td>0.183013</td>
      <td>0.128657</td>
      <td>0.132217</td>
      <td>0.327314</td>
      <td>0.610225</td>
      <td>0.610225</td>
      <td>0.256114</td>
      <td>-0.673593</td>
      <td>0.346015</td>
      <td>-0.727943</td>
      <td>-0.712706</td>
      <td>0.649693</td>
      <td>0.300343</td>
      <td>-0.602987</td>
      <td>-0.419466</td>
      <td>-0.211766</td>
      <td>-0.154846</td>
      <td>-0.121134</td>
      <td>0.204411</td>
      <td>0.248762</td>
      <td>0.254086</td>
      <td>-0.725167</td>
      <td>0.871444</td>
      <td>-0.182734</td>
      <td>0.219927</td>
      <td>0.871444</td>
      <td>0.046297</td>
      <td>-0.725167</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>0.230215</td>
      <td>0.222894</td>
      <td>0.344802</td>
      <td>-0.381963</td>
      <td>0.679154</td>
      <td>0.708169</td>
      <td>-0.681095</td>
      <td>0.645312</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.010653</td>
      <td>0.011959</td>
      <td>-0.009079</td>
      <td>-0.011793</td>
      <td>-0.030028</td>
      <td>0.378184</td>
      <td>-0.378184</td>
      <td>-0.257165</td>
      <td>0.243475</td>
      <td>0.295122</td>
      <td>0.011312</td>
      <td>-0.707262</td>
      <td>1.294470</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.544050</td>
      <td>-0.568366</td>
      <td>0.263737</td>
      <td>0.454816</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>-0.662387</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>26415</th>
      <td>0.328346</td>
      <td>0.273404</td>
      <td>0.426766</td>
      <td>0.368510</td>
      <td>0.352879</td>
      <td>-0.142157</td>
      <td>0.328346</td>
      <td>0.363700</td>
      <td>0.211629</td>
      <td>0.282207</td>
      <td>0.165440</td>
      <td>0.168781</td>
      <td>0.426766</td>
      <td>0.610225</td>
      <td>0.610225</td>
      <td>0.355109</td>
      <td>-0.684832</td>
      <td>0.346015</td>
      <td>-0.775006</td>
      <td>-0.737543</td>
      <td>0.636647</td>
      <td>0.399333</td>
      <td>-0.614821</td>
      <td>-0.429921</td>
      <td>-0.219665</td>
      <td>-0.159004</td>
      <td>-0.117075</td>
      <td>0.298132</td>
      <td>0.347542</td>
      <td>0.353051</td>
      <td>-0.712106</td>
      <td>0.860396</td>
      <td>-0.188272</td>
      <td>0.209666</td>
      <td>0.860396</td>
      <td>0.052962</td>
      <td>-0.712106</td>
      <td>0.780527</td>
      <td>1.013760</td>
      <td>-1.0</td>
      <td>...</td>
      <td>0.328346</td>
      <td>0.321992</td>
      <td>0.443535</td>
      <td>-0.370362</td>
      <td>0.663225</td>
      <td>0.705943</td>
      <td>-0.665124</td>
      <td>0.628902</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>0.009425</td>
      <td>0.010642</td>
      <td>0.011983</td>
      <td>-0.009072</td>
      <td>-0.011806</td>
      <td>-0.029848</td>
      <td>0.366473</td>
      <td>-0.366473</td>
      <td>-0.356309</td>
      <td>0.342588</td>
      <td>0.393953</td>
      <td>0.011426</td>
      <td>-0.695865</td>
      <td>1.294470</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.550341</td>
      <td>-0.570796</td>
      <td>0.217452</td>
      <td>0.514896</td>
      <td>0.009425</td>
      <td>-0.009477</td>
      <td>-0.662387</td>
      <td>0.695654</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
    <tr>
      <th>26416</th>
      <td>0.665220</td>
      <td>0.578979</td>
      <td>0.660973</td>
      <td>0.607246</td>
      <td>0.724351</td>
      <td>0.453354</td>
      <td>0.665220</td>
      <td>0.646963</td>
      <td>-0.329597</td>
      <td>0.573600</td>
      <td>0.225900</td>
      <td>0.240342</td>
      <td>0.660973</td>
      <td>0.610225</td>
      <td>0.610225</td>
      <td>0.652096</td>
      <td>-0.363127</td>
      <td>1.149308</td>
      <td>-0.772199</td>
      <td>0.157143</td>
      <td>0.689135</td>
      <td>0.687145</td>
      <td>0.044559</td>
      <td>0.494054</td>
      <td>0.795467</td>
      <td>0.719605</td>
      <td>0.201816</td>
      <td>0.626383</td>
      <td>0.652291</td>
      <td>0.652359</td>
      <td>-0.568812</td>
      <td>0.054844</td>
      <td>1.051002</td>
      <td>1.024844</td>
      <td>0.054844</td>
      <td>-1.078105</td>
      <td>-0.568812</td>
      <td>0.780527</td>
      <td>-0.986427</td>
      <td>-1.0</td>
      <td>...</td>
      <td>0.665220</td>
      <td>0.644576</td>
      <td>0.648737</td>
      <td>-0.950350</td>
      <td>-0.177657</td>
      <td>0.253962</td>
      <td>0.178026</td>
      <td>-0.208844</td>
      <td>-0.006153</td>
      <td>-0.006153</td>
      <td>0.077322</td>
      <td>1.408142</td>
      <td>0.010732</td>
      <td>0.012557</td>
      <td>-0.010102</td>
      <td>-0.011377</td>
      <td>-0.025733</td>
      <td>0.948959</td>
      <td>-0.948959</td>
      <td>-0.493378</td>
      <td>0.592196</td>
      <td>0.725212</td>
      <td>-0.003679</td>
      <td>-1.247369</td>
      <td>0.145295</td>
      <td>-0.063096</td>
      <td>-0.018211</td>
      <td>-0.401826</td>
      <td>-0.762387</td>
      <td>-0.228541</td>
      <td>1.156674</td>
      <td>1.408142</td>
      <td>-1.408143</td>
      <td>-0.662387</td>
      <td>-1.437496</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.243792</td>
    </tr>
  </tbody>
</table>
<p>26417 rows × 123 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># use this function to find variance captured in n PCs</span>
<span class="c1"># ideally we&#39;d like 75-90% variance captured in 2 or 3 PCs</span>
<span class="c1"># to give an accurate representation of the original data</span>

<span class="k">def</span> <span class="nf">ideal_pca</span><span class="p">(</span><span class="n">df_std</span><span class="p">):</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># or use len(df_std.columns)</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_std</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">variance</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">variance</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;% Variance Explained&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;# of Principal Components&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PCA Analysis&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">100.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
    
<span class="n">ideal_pca</span><span class="p">(</span><span class="n">df1_std</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_70_0.png" src="_images/bckup_1classification_70_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pca3</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df1_std</span><span class="p">)</span>
<span class="n">pca3d</span> <span class="o">=</span> <span class="n">pca3</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df1_std</span><span class="p">)</span>
<span class="n">pca3d_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca3d</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pca3</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total variance captured:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">pca3</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="c1"># the % variance captured by our PCs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.32182321 0.21781189 0.13619469 0.0699446  0.0350436 ]

Total variance captured:

0.7808179870194095
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pca3d_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.479781</td>
      <td>2.196632</td>
      <td>0.555395</td>
      <td>-4.694193</td>
      <td>1.827663</td>
    </tr>
    <tr>
      <th>1</th>
      <td>23.148063</td>
      <td>1.351246</td>
      <td>-0.376630</td>
      <td>-4.838082</td>
      <td>1.933818</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.213829</td>
      <td>7.570060</td>
      <td>5.841912</td>
      <td>-3.278936</td>
      <td>6.812912</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11.664291</td>
      <td>9.782295</td>
      <td>-2.254102</td>
      <td>-3.964441</td>
      <td>3.505594</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12.299023</td>
      <td>9.864997</td>
      <td>-2.051454</td>
      <td>-3.971385</td>
      <td>3.628099</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>26412</th>
      <td>3.462305</td>
      <td>-4.156793</td>
      <td>-2.779734</td>
      <td>-1.325572</td>
      <td>0.161441</td>
    </tr>
    <tr>
      <th>26413</th>
      <td>5.243921</td>
      <td>-3.735498</td>
      <td>0.654760</td>
      <td>-1.911027</td>
      <td>-1.356353</td>
    </tr>
    <tr>
      <th>26414</th>
      <td>1.330057</td>
      <td>-3.492736</td>
      <td>0.889212</td>
      <td>-2.119469</td>
      <td>-1.253958</td>
    </tr>
    <tr>
      <th>26415</th>
      <td>1.926167</td>
      <td>-3.536070</td>
      <td>0.846977</td>
      <td>-2.105734</td>
      <td>-1.295859</td>
    </tr>
    <tr>
      <th>26416</th>
      <td>3.939631</td>
      <td>-2.701668</td>
      <td>3.972541</td>
      <td>4.016749</td>
      <td>1.124536</td>
    </tr>
  </tbody>
</table>
<p>26417 rows × 5 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">pca3d_df</span><span class="p">,</span> <span class="n">e_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id12">
<h2>Decision Tree Classifier<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the feature importance according to this Decision Tree Model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;importance</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">importance</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.69 (+/- 0.01)

The list of features with importance: 
[&#39;id&#39;, &#39;time&#39;, &#39;value__variance_larger_than_standard_deviation&#39;, &#39;value__has_duplicate_max&#39;, &#39;value__has_duplicate_min&#39;]
importance
 [0.16368477 0.15996109 0.24157696 0.2176182  0.21715897]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id13">
<h2>AutoML<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.7309613928841786
</pre></div>
</div>
</div>
</div>
<p>We decided that for our situation, with training not taking particularly long without PCA and losing more than 20% of the variance with PCA, it wasn’t the right way to go and instead we would continue to simply use the full amount of features</p>
<p>#Binary Data Classifier</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># first we&#39;ll create a binary label set, indicating 0 for awake and 1 for asleep</span>

<span class="n">e_binary_label_array</span> <span class="o">=</span> <span class="n">extracted_labels</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">e_binary_label_array</span><span class="p">[</span><span class="n">e_binary_label_array</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">e_binary_label_array</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">e_binary_label_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 1]
(26417,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted Features&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">extracted_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Extracted Labels&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">e_binary_label_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracted Features 
             id  ...  value__permutation_entropy__dimension_4__tau_1
0        46343  ...                                            -0.0
1        46343  ...                                            -0.0
2        46343  ...                                            -0.0
3        46343  ...                                            -0.0
4        46343  ...                                            -0.0
...        ...  ...                                             ...
26412  9961348  ...                                            -0.0
26413  9961348  ...                                            -0.0
26414  9961348  ...                                            -0.0
26415  9961348  ...                                            -0.0
26416  9961348  ...                                            -0.0

[26417 rows x 271 columns]

Extracted Labels 
 [0 0 0 ... 0 0 0]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">extracted_features</span><span class="p">,</span> <span class="n">e_binary_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id14">
<h2>Decision Tree Classifier<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the feature importance according to this Decision Tree Model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="c1">#print(&quot;importance\n&quot;, importance)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.94 (+/- 0.00)

The list of features with importance: 
[&#39;id&#39;, &#39;time&#39;, &#39;value__sum_values&#39;, &#39;value__abs_energy&#39;, &#39;value__mean_abs_change&#39;, &#39;value__mean_change&#39;, &#39;value__mean_second_derivative_central&#39;, &#39;value__median&#39;, &#39;value__mean&#39;, &#39;value__standard_deviation&#39;, &#39;value__variation_coefficient&#39;, &#39;value__variance&#39;, &#39;value__skewness&#39;, &#39;value__kurtosis&#39;, &#39;value__absolute_sum_of_changes&#39;, &#39;value__last_location_of_minimum&#39;, &#39;value__maximum&#39;, &#39;value__minimum&#39;, &#39;value__benford_correlation&#39;, &#39;value__time_reversal_asymmetry_statistic__lag_1&#39;, &#39;value__c3__lag_1&#39;, &#39;value__cid_ce__normalize_True&#39;, &#39;value__cid_ce__normalize_False&#39;, &#39;value__quantile__q_0.1&#39;, &#39;value__quantile__q_0.2&#39;, &#39;value__quantile__q_0.3&#39;, &#39;value__quantile__q_0.4&#39;, &#39;value__quantile__q_0.6&#39;, &#39;value__quantile__q_0.7&#39;, &#39;value__quantile__q_0.8&#39;, &#39;value__quantile__q_0.9&#39;, &#39;value__autocorrelation__lag_1&#39;, &#39;value__autocorrelation__lag_2&#39;, &#39;value__autocorrelation__lag_3&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;mean&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;median&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;var&quot;__maxlag_40&#39;, &#39;value__partial_autocorrelation__lag_1&#39;, &#39;value__cwt_coefficients__coeff_0__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__spkt_welch_density__coeff_2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.4&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.6&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;imag&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;angle&quot;__coeff_1&#39;, &#39;value__fft_aggregated__aggtype_&quot;centroid&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;variance&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;skew&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;kurtosis&quot;&#39;, &#39;value__range_count__max_1__min_-1&#39;, &#39;value__friedrich_coefficients__coeff_0__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_1__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_2__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_3__m_3__r_30&#39;, &#39;value__max_langevin_fixed_point__m_3__r_30&#39;, &#39;value__linear_trend__attr_&quot;pvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;rvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;intercept&quot;&#39;, &#39;value__linear_trend__attr_&quot;slope&quot;&#39;, &#39;value__linear_trend__attr_&quot;stderr&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;teststat&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;pvalue&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__number_crossing_m__m_0&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_0&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_1&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_2&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_3&#39;, &#39;value__lempel_ziv_complexity__bins_100&#39;, &#39;value__permutation_entropy__dimension_3__tau_1&#39;]
</pre></div>
</div>
</div>
</div>
<p>##Nested Cross-Validation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
<p>##Comparing Classification Algorithms</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.9081873407869206 0.009351711019187057

Test set accuracy:
 0.9077971233913702 

Confusion Matrix
 [[   0  609]
 [   0 5996]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       609
           1       0.91      1.00      0.95      5996

    accuracy                           0.91      6605
   macro avg       0.45      0.50      0.48      6605
weighted avg       0.82      0.91      0.86      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.9084901671836025 0.00921647142524848

Test set accuracy:
 0.9080999242997729 

Confusion Matrix
 [[   3  606]
 [   1 5995]] 

Classification report
               precision    recall  f1-score   support

           0       0.75      0.00      0.01       609
           1       0.91      1.00      0.95      5996

    accuracy                           0.91      6605
   macro avg       0.83      0.50      0.48      6605
weighted avg       0.89      0.91      0.86      6605
 

Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.9625483974651214 0.008226771447011855

Test set accuracy:
 0.9647236941710825 

Confusion Matrix
 [[ 418  191]
 [  42 5954]] 

Classification report
               precision    recall  f1-score   support

           0       0.91      0.69      0.78       609
           1       0.97      0.99      0.98      5996

    accuracy                           0.96      6605
   macro avg       0.94      0.84      0.88      6605
weighted avg       0.96      0.96      0.96      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.3533698287107949 0.41296958333771544

Test set accuracy:
 0.3821347464042392 

Confusion Matrix
 [[ 526   83]
 [3998 1998]] 

Classification report
               precision    recall  f1-score   support

           0       0.12      0.86      0.20       609
           1       0.96      0.33      0.49      5996

    accuracy                           0.38      6605
   macro avg       0.54      0.60      0.35      6605
weighted avg       0.88      0.38      0.47      6605
 

Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.9385731553695524 0.012195058698595688

Test set accuracy:
 0.9342922028766086 

Confusion Matrix
 [[ 373  236]
 [ 198 5798]] 

Classification report
               precision    recall  f1-score   support

           0       0.65      0.61      0.63       609
           1       0.96      0.97      0.96      5996

    accuracy                           0.93      6605
   macro avg       0.81      0.79      0.80      6605
weighted avg       0.93      0.93      0.93      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.9506363429370136 0.00787252245719144

Test set accuracy:
 0.9494322482967449 

Confusion Matrix
 [[ 323  286]
 [  48 5948]] 

Classification report
               precision    recall  f1-score   support

           0       0.87      0.53      0.66       609
           1       0.95      0.99      0.97      5996

    accuracy                           0.95      6605
   macro avg       0.91      0.76      0.82      6605
weighted avg       0.95      0.95      0.94      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.908136835762142 0.00922736732173412

Test set accuracy:
 0.9077971233913702 

Confusion Matrix
 [[   0  609]
 [   0 5996]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       609
           1       0.91      1.00      0.95      5996

    accuracy                           0.91      6605
   macro avg       0.45      0.50      0.48      6605
weighted avg       0.82      0.91      0.86      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.9282256614426355 0.0072063281722145305

Test set accuracy:
 0.9274791824375473 

Confusion Matrix
 [[ 193  416]
 [  63 5933]] 

Classification report
               precision    recall  f1-score   support

           0       0.75      0.32      0.45       609
           1       0.93      0.99      0.96      5996

    accuracy                           0.93      6605
   macro avg       0.84      0.65      0.70      6605
weighted avg       0.92      0.93      0.91      6605
 
</pre></div>
</div>
</div>
</div>
<p>##Plotting Algorithm Performance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Wake/Sleep Label Data With Extracted Features&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_90_0.png" src="_images/bckup_1classification_90_0.png" />
</div>
</div>
<p>Classification on binary labelled data with extracted features yielded improved results in accuracy compared to that in the independent data point model earlier in the notebook. As shown, with the exception of Naive Bayes, all the algorithms displayed accuracy of above 90%. Decision Tree and Random Forest performed well again but K-nearest neighbours had the highest accuracy for this dataset with an accuracy of 96.1%. Upon inspection of the confusion matrix, there is no indication of the model guessing “Sleep” for all data points as shown in the false predictions for “Wake”.</p>
<p>##AutoML</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.9538228614685844
</pre></div>
</div>
</div>
</div>
<p>#Three-Label Classifier
Now we are using the extracted feature set to classify based on the three label categories of Wake/Non-REM Sleep/REM Sleep and again assigning them the labels of 0/1/2 respectively</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">e_three_label_array</span> <span class="o">=</span> <span class="n">extracted_labels</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">e_three_label_array</span><span class="p">[(</span><span class="n">e_three_label_array</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">e_three_label_array</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">e_three_label_array</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">e_three_label_array</span><span class="p">[</span><span class="n">e_three_label_array</span> <span class="o">==</span> <span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>

<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">three_label_array</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">three_label_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 1 2]
(26417,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">extracted_features</span><span class="p">,</span> <span class="n">e_three_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>##Decision Tree Classifier</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="c1">#print(&quot;importance\n&quot;, importance)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.88 (+/- 0.01)

The list of features with importance: 
[&#39;id&#39;, &#39;time&#39;, &#39;value__sum_values&#39;, &#39;value__abs_energy&#39;, &#39;value__mean_abs_change&#39;, &#39;value__mean_change&#39;, &#39;value__mean_second_derivative_central&#39;, &#39;value__median&#39;, &#39;value__mean&#39;, &#39;value__standard_deviation&#39;, &#39;value__variation_coefficient&#39;, &#39;value__variance&#39;, &#39;value__skewness&#39;, &#39;value__kurtosis&#39;, &#39;value__absolute_sum_of_changes&#39;, &#39;value__last_location_of_minimum&#39;, &#39;value__maximum&#39;, &#39;value__minimum&#39;, &#39;value__benford_correlation&#39;, &#39;value__time_reversal_asymmetry_statistic__lag_1&#39;, &#39;value__c3__lag_1&#39;, &#39;value__cid_ce__normalize_True&#39;, &#39;value__cid_ce__normalize_False&#39;, &#39;value__quantile__q_0.1&#39;, &#39;value__quantile__q_0.2&#39;, &#39;value__quantile__q_0.3&#39;, &#39;value__quantile__q_0.4&#39;, &#39;value__quantile__q_0.6&#39;, &#39;value__quantile__q_0.7&#39;, &#39;value__quantile__q_0.8&#39;, &#39;value__quantile__q_0.9&#39;, &#39;value__autocorrelation__lag_1&#39;, &#39;value__autocorrelation__lag_2&#39;, &#39;value__autocorrelation__lag_3&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;mean&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;median&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;var&quot;__maxlag_40&#39;, &#39;value__partial_autocorrelation__lag_1&#39;, &#39;value__number_cwt_peaks__n_1&#39;, &#39;value__cwt_coefficients__coeff_0__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__spkt_welch_density__coeff_2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.4&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.4&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.6&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.6&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;imag&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;angle&quot;__coeff_1&#39;, &#39;value__fft_aggregated__aggtype_&quot;centroid&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;variance&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;skew&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;kurtosis&quot;&#39;, &#39;value__range_count__max_1000000000000.0__min_0&#39;, &#39;value__friedrich_coefficients__coeff_0__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_1__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_2__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_3__m_3__r_30&#39;, &#39;value__max_langevin_fixed_point__m_3__r_30&#39;, &#39;value__linear_trend__attr_&quot;pvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;rvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;intercept&quot;&#39;, &#39;value__linear_trend__attr_&quot;slope&quot;&#39;, &#39;value__linear_trend__attr_&quot;stderr&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;teststat&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;pvalue&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_0&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_1&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_2&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_3&#39;, &#39;value__count_above__t_0&#39;, &#39;value__lempel_ziv_complexity__bins_100&#39;]
</pre></div>
</div>
</div>
</div>
<p>##Nested Cross-Validation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
<p>##Comparing Classification Algorithms</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.6772655311228619 0.019951434662157083

Test set accuracy:
 0.6852384557153671 

Confusion Matrix
 [[   0  587    8]
 [   0 4497   67]
 [   0 1417   29]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       595
           1       0.69      0.99      0.81      4564
           2       0.28      0.02      0.04      1446

    accuracy                           0.69      6605
   macro avg       0.32      0.34      0.28      6605
weighted avg       0.54      0.69      0.57      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.6842815271822984 0.01975274575622416

Test set accuracy:
 0.6915972747918244 

Confusion Matrix
 [[   1  592    2]
 [   0 4561    3]
 [   0 1440    6]] 

Classification report
               precision    recall  f1-score   support

           0       1.00      0.00      0.00       595
           1       0.69      1.00      0.82      4564
           2       0.55      0.00      0.01      1446

    accuracy                           0.69      6605
   macro avg       0.75      0.34      0.28      6605
weighted avg       0.69      0.69      0.57      6605
 

Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.9225717983812922 0.009332787532341515

Test set accuracy:
 0.9262679788039364 

Confusion Matrix
 [[ 375  186   34]
 [  44 4401  119]
 [   4  100 1342]] 

Classification report
               precision    recall  f1-score   support

           0       0.89      0.63      0.74       595
           1       0.94      0.96      0.95      4564
           2       0.90      0.93      0.91      1446

    accuracy                           0.93      6605
   macro avg       0.91      0.84      0.87      6605
weighted avg       0.93      0.93      0.92      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.2616646741419876 0.24510005578294214

Test set accuracy:
 0.21604844814534443 

Confusion Matrix
 [[ 136    4  455]
 [ 593   40 3931]
 [ 185   10 1251]] 

Classification report
               precision    recall  f1-score   support

           0       0.15      0.23      0.18       595
           1       0.74      0.01      0.02      4564
           2       0.22      0.87      0.35      1446

    accuracy                           0.22      6605
   macro avg       0.37      0.37      0.18      6605
weighted avg       0.57      0.22      0.11      6605
 

Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.8846147126256451 0.013923585817119234

Test set accuracy:
 0.8870552611657835 

Confusion Matrix
 [[ 380  173   42]
 [ 136 4256  172]
 [  46  177 1223]] 

Classification report
               precision    recall  f1-score   support

           0       0.68      0.64      0.66       595
           1       0.92      0.93      0.93      4564
           2       0.85      0.85      0.85      1446

    accuracy                           0.89      6605
   macro avg       0.82      0.81      0.81      6605
weighted avg       0.89      0.89      0.89      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.8934482273831469 0.012755991963193132

Test set accuracy:
 0.8928084784254353 

Confusion Matrix
 [[ 355  201   39]
 [  53 4360  151]
 [  19  245 1182]] 

Classification report
               precision    recall  f1-score   support

           0       0.83      0.60      0.69       595
           1       0.91      0.96      0.93      4564
           2       0.86      0.82      0.84      1446

    accuracy                           0.89      6605
   macro avg       0.87      0.79      0.82      6605
weighted avg       0.89      0.89      0.89      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.6834234511410365 0.020022817085661705

Test set accuracy:
 0.690991672975019 

Confusion Matrix
 [[   0  595    0]
 [   0 4564    0]
 [   0 1446    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       595
           1       0.69      1.00      0.82      4564
           2       0.00      0.00      0.00      1446

    accuracy                           0.69      6605
   macro avg       0.23      0.33      0.27      6605
weighted avg       0.48      0.69      0.56      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.714616964085146 0.019613024246668487

Test set accuracy:
 0.7261165783497351 

Confusion Matrix
 [[ 216  343   36]
 [ 110 4336  118]
 [   6 1196  244]] 

Classification report
               precision    recall  f1-score   support

           0       0.65      0.36      0.47       595
           1       0.74      0.95      0.83      4564
           2       0.61      0.17      0.26      1446

    accuracy                           0.73      6605
   macro avg       0.67      0.49      0.52      6605
weighted avg       0.70      0.73      0.67      6605
 
</pre></div>
</div>
</div>
</div>
<p>##Plotting Algorithm Performance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Wake/NREM/REM Label Data With Extracted Features&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_104_0.png" src="_images/bckup_1classification_104_0.png" />
</div>
</div>
<p>The models’ accuracies decrease as another label is added to the classification. K-Nearest Neighbours is the only model to achieve an accuracy of &gt; 0.9, with Decision Tree and RF closely following behind it.</p>
<p>##AutoML</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[WARNING] [2020-11-20 13:17:10,991:AutoML(1):4a90586aeebb8ab627a0bcbde461d7de] No valid ensemble was created. Please check the logfile for errors. Default to the best individual estimator:[(1, 3, 0.0)]
Test Accuracy score 0.9015897047691143
</pre></div>
</div>
</div>
</div>
<p>#Multi-Label Classifier</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">e_label_array</span> <span class="o">=</span> <span class="n">extracted_labels</span><span class="p">[</span><span class="s1">&#39;1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">e_label_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">e_label_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(26417,)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">extracted_features</span> <span class="o">=</span> <span class="n">extracted_features</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">extracted_labels</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted Features&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">extracted_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Extracted Labels&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">extracted_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracted Features 
             id  ...  value__permutation_entropy__dimension_4__tau_1
0        46343  ...                                            -0.0
1        46343  ...                                            -0.0
2        46343  ...                                            -0.0
3        46343  ...                                            -0.0
4        46343  ...                                            -0.0
...        ...  ...                                             ...
26412  9961348  ...                                            -0.0
26413  9961348  ...                                            -0.0
26414  9961348  ...                                            -0.0
26415  9961348  ...                                            -0.0
26416  9961348  ...                                            -0.0

[26417 rows x 271 columns]

Extracted Labels 
             id   time  1
0        46343    390  0
1        46343    420  0
2        46343    450  0
3        46343    480  0
4        46343    510  0
...        ...    ... ..
26412  9961348  21450  0
26413  9961348  21480  0
26414  9961348  21510  0
26415  9961348  21540  0
26416  9961348  21570  0

[26417 rows x 3 columns]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Splitting our data into training and testing.</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">extracted_features</span><span class="p">,</span> <span class="n">e_label_array</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id15">
<h2>Decision Tree Classifier<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<p>Let’s look at the feature importance according to this Decision Tree Model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The mean score and the 95</span><span class="si">% c</span><span class="s1">onfidence interval of the score estimate are given by:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Training the model in a standard way allows to to view the feature importance</span>
<span class="c1"># for this model alone.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">importance</span> <span class="o">=</span> <span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># using list comprehension + enumerate() </span>
<span class="c1"># index of matching element </span>
<span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> 
<span class="n">res_feats</span> <span class="o">=</span> <span class="p">[</span><span class="n">extracted_features</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="p">]</span>
  
<span class="c1"># print result </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">The list of features with importance: &quot;</span><span class="p">)</span> 

<span class="nb">print</span><span class="p">(</span><span class="n">res_feats</span><span class="p">)</span>
<span class="c1">#print(&quot;importance\n&quot;, importance)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean score and the 95% confidence interval of the score estimate are given by:
Accuracy: 0.80 (+/- 0.02)

The list of features with importance: 
[&#39;id&#39;, &#39;time&#39;, &#39;value__sum_values&#39;, &#39;value__abs_energy&#39;, &#39;value__mean_abs_change&#39;, &#39;value__mean_change&#39;, &#39;value__mean_second_derivative_central&#39;, &#39;value__median&#39;, &#39;value__mean&#39;, &#39;value__standard_deviation&#39;, &#39;value__variation_coefficient&#39;, &#39;value__variance&#39;, &#39;value__skewness&#39;, &#39;value__kurtosis&#39;, &#39;value__absolute_sum_of_changes&#39;, &#39;value__last_location_of_minimum&#39;, &#39;value__first_location_of_minimum&#39;, &#39;value__maximum&#39;, &#39;value__minimum&#39;, &#39;value__benford_correlation&#39;, &#39;value__time_reversal_asymmetry_statistic__lag_1&#39;, &#39;value__c3__lag_1&#39;, &#39;value__cid_ce__normalize_True&#39;, &#39;value__cid_ce__normalize_False&#39;, &#39;value__quantile__q_0.1&#39;, &#39;value__quantile__q_0.2&#39;, &#39;value__quantile__q_0.3&#39;, &#39;value__quantile__q_0.4&#39;, &#39;value__quantile__q_0.6&#39;, &#39;value__quantile__q_0.7&#39;, &#39;value__quantile__q_0.8&#39;, &#39;value__quantile__q_0.9&#39;, &#39;value__autocorrelation__lag_1&#39;, &#39;value__autocorrelation__lag_2&#39;, &#39;value__autocorrelation__lag_3&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;mean&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;median&quot;__maxlag_40&#39;, &#39;value__agg_autocorrelation__f_agg_&quot;var&quot;__maxlag_40&#39;, &#39;value__partial_autocorrelation__lag_1&#39;, &#39;value__number_cwt_peaks__n_1&#39;, &#39;value__number_peaks__n_1&#39;, &#39;value__cwt_coefficients__coeff_0__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_1__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_2__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_5__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_10__widths_(2, 5, 10, 20)&#39;, &#39;value__cwt_coefficients__coeff_3__w_20__widths_(2, 5, 10, 20)&#39;, &#39;value__spkt_welch_density__coeff_2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.4__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.6__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_0.8__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.0&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_0.8__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_False__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;var&quot;__isabs_True__qh_1.0__ql_0.2&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.4&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_False__qh_1.0__ql_0.6&#39;, &#39;value__change_quantiles__f_agg_&quot;mean&quot;__isabs_True__qh_1.0__ql_0.6&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;real&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;imag&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_0&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_1&#39;, &#39;value__fft_coefficient__attr_&quot;abs&quot;__coeff_2&#39;, &#39;value__fft_coefficient__attr_&quot;angle&quot;__coeff_1&#39;, &#39;value__fft_aggregated__aggtype_&quot;centroid&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;variance&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;skew&quot;&#39;, &#39;value__fft_aggregated__aggtype_&quot;kurtosis&quot;&#39;, &#39;value__range_count__max_1000000000000.0__min_0&#39;, &#39;value__friedrich_coefficients__coeff_0__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_1__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_2__m_3__r_30&#39;, &#39;value__friedrich_coefficients__coeff_3__m_3__r_30&#39;, &#39;value__max_langevin_fixed_point__m_3__r_30&#39;, &#39;value__linear_trend__attr_&quot;pvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;rvalue&quot;&#39;, &#39;value__linear_trend__attr_&quot;intercept&quot;&#39;, &#39;value__linear_trend__attr_&quot;slope&quot;&#39;, &#39;value__linear_trend__attr_&quot;stderr&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;teststat&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__augmented_dickey_fuller__attr_&quot;pvalue&quot;__autolag_&quot;AIC&quot;&#39;, &#39;value__number_crossing_m__m_0&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_0&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_1&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_2&#39;, &#39;value__energy_ratio_by_chunks__num_segments_10__segment_focus_3&#39;, &#39;value__count_above__t_0&#39;, &#39;value__lempel_ziv_complexity__bins_100&#39;]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id16">
<h2>Nested Cross Validation<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#nested_cross_val(X_train, y_train)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id17">
<h2>Comparing Classification Algorithms<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">names</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">algorithm_comparison</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
LOGISTIC REGRESSION: 0.4917209708171117 0.021632213103959075
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set accuracy:
 0.48902346707040123 

Confusion Matrix
 [[   0    0  460  153    8]
 [   0    0  413   16    5]
 [   0    0 3090   67   74]
 [   0    0  774   87    0]
 [   0    0 1400    5   53]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       621
           1       0.00      0.00      0.00       434
           2       0.50      0.96      0.66      3231
           3       0.27      0.10      0.15       861
           5       0.38      0.04      0.07      1458

    accuracy                           0.49      6605
   macro avg       0.23      0.22      0.17      6605
weighted avg       0.36      0.49      0.36      6605
 

Model  Mean Accuracy  95% conf interval
LINEAR DISCRIMINANT AANALYSIS: 0.49666819650453276 0.01435684270836671

Test set accuracy:
 0.49613928841786525 

Confusion Matrix
 [[  15    0  598    2    6]
 [   2    2  425    2    3]
 [   3    3 3224    0    1]
 [   0    0  855    6    0]
 [   1    1 1426    0   30]] 

Classification report
               precision    recall  f1-score   support

           0       0.71      0.02      0.05       621
           1       0.33      0.00      0.01       434
           2       0.49      1.00      0.66      3231
           3       0.60      0.01      0.01       861
           5       0.75      0.02      0.04      1458

    accuracy                           0.50      6605
   macro avg       0.58      0.21      0.15      6605
weighted avg       0.57      0.50      0.34      6605
 

Model  Mean Accuracy  95% conf interval
K NEAREST NEIGHBOURS: 0.8588224357429894 0.01319212759270069

Test set accuracy:
 0.8607115821347464 

Confusion Matrix
 [[ 421   58   97   13   32]
 [  33  142  194    8   57]
 [  17   70 3001   68   75]
 [   1    0   66  792    2]
 [   2   21  102    4 1329]] 

Classification report
               precision    recall  f1-score   support

           0       0.89      0.68      0.77       621
           1       0.49      0.33      0.39       434
           2       0.87      0.93      0.90      3231
           3       0.89      0.92      0.91       861
           5       0.89      0.91      0.90      1458

    accuracy                           0.86      6605
   macro avg       0.81      0.75      0.77      6605
weighted avg       0.85      0.86      0.85      6605
 

Model  Mean Accuracy  95% conf interval
NAIVE BAYES: 0.16528998238054657 0.20373172448389462

Test set accuracy:
 0.11839515518546556 

Confusion Matrix
 [[  52  403    4  160    2]
 [  36  307    4   86    1]
 [  97 2501   26  599    8]
 [  54  405   14  388    0]
 [  71 1120   13  245    9]] 

Classification report
               precision    recall  f1-score   support

           0       0.17      0.08      0.11       621
           1       0.06      0.71      0.12       434
           2       0.43      0.01      0.02      3231
           3       0.26      0.45      0.33       861
           5       0.45      0.01      0.01      1458

    accuracy                           0.12      6605
   macro avg       0.27      0.25      0.12      6605
weighted avg       0.36      0.12      0.07      6605
 

Model  Mean Accuracy  95% conf interval
DECISION TREE: 0.8055718783539488 0.018112787825674617

Test set accuracy:
 0.8148372445117336 

Confusion Matrix
 [[ 380   78  100   15   48]
 [  39  163  178   10   44]
 [  71  148 2824   95   93]
 [   9    3  106  740    3]
 [  28   43  104    8 1275]] 

Classification report
               precision    recall  f1-score   support

           0       0.72      0.61      0.66       621
           1       0.37      0.38      0.38       434
           2       0.85      0.87      0.86      3231
           3       0.85      0.86      0.86       861
           5       0.87      0.87      0.87      1458

    accuracy                           0.81      6605
   macro avg       0.73      0.72      0.73      6605
weighted avg       0.81      0.81      0.81      6605
 

Model  Mean Accuracy  95% conf interval
RANDOM FOREST: 0.8119820178680308 0.0146827354912632

Test set accuracy:
 0.8119606358819077 

Confusion Matrix
 [[ 365   54  130   10   62]
 [  43  110  216    9   56]
 [  30   72 2921   99  109]
 [   6    1  128  714   12]
 [  16   17  164    8 1253]] 

Classification report
               precision    recall  f1-score   support

           0       0.79      0.59      0.68       621
           1       0.43      0.25      0.32       434
           2       0.82      0.90      0.86      3231
           3       0.85      0.83      0.84       861
           5       0.84      0.86      0.85      1458

    accuracy                           0.81      6605
   macro avg       0.75      0.69      0.71      6605
weighted avg       0.80      0.81      0.80      6605
 

Model  Mean Accuracy  95% conf interval
SUPPORT VECTOR MACHINE: 0.4907627506722543 0.014620731312731344

Test set accuracy:
 0.4891748675246026 

Confusion Matrix
 [[   0    0  621    0    0]
 [   0    0  434    0    0]
 [   0    0 3231    0    0]
 [   0    0  861    0    0]
 [   0    0 1458    0    0]] 

Classification report
               precision    recall  f1-score   support

           0       0.00      0.00      0.00       621
           1       0.00      0.00      0.00       434
           2       0.49      1.00      0.66      3231
           3       0.00      0.00      0.00       861
           5       0.00      0.00      0.00      1458

    accuracy                           0.49      6605
   macro avg       0.10      0.20      0.13      6605
weighted avg       0.24      0.49      0.32      6605
 
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model  Mean Accuracy  95% conf interval
ADABOOST CLASSIFIER: 0.5070655842002556 0.02290260909353689

Test set accuracy:
 0.506888720666162 

Confusion Matrix
 [[ 160    9  334   16  102]
 [  36    8  273   25   92]
 [  73   14 2583  209  352]
 [   5    0  522  276   58]
 [  15    6 1054   62  321]] 

Classification report
               precision    recall  f1-score   support

           0       0.55      0.26      0.35       621
           1       0.22      0.02      0.03       434
           2       0.54      0.80      0.65      3231
           3       0.47      0.32      0.38       861
           5       0.35      0.22      0.27      1458

    accuracy                           0.51      6605
   macro avg       0.43      0.32      0.34      6605
weighted avg       0.47      0.51      0.46      6605
 
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id18">
<h2>Plotting Algorithm Performance<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plot_algo_comparison</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Algorithm Comparison on Multi-Label Data With Extracted Features&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/bckup_1classification_120_0.png" src="_images/bckup_1classification_120_0.png" />
</div>
</div>
<p>As expected, the models’ accuracies deteriorate again with the inclusion of the full label to the classification. Again, Decision Tree and Random Forest return high accuracies of ~0.8 but K-Nearest Neighbours is the most accurate with a score of 0.86.</p>
</div>
<div class="section" id="id19">
<h2>AutoML<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">autosklearn.classification</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="c1"># configure auto-sklearn</span>
<span class="n">automl</span> <span class="o">=</span> <span class="n">autosklearn</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">AutoSklearnClassifier</span><span class="p">(</span>
          <span class="n">time_left_for_this_task</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="c1"># run auto-sklearn for at most 2min</span>
          <span class="n">per_run_time_limit</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="c1"># spend at most 30 sec for each model training</span>
          <span class="p">)</span>

<span class="c1"># train model(s)</span>
<span class="n">automl</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># evaluate</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">automl</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Accuracy score </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Accuracy score 0.09401968205904618
</pre></div>
</div>
</div>
</div>
<p>#Evaluation of Model Performances
As expected, we observe an improvement of model accuracy with addition of time series as opposed to taking each window as independent data points. An important note is that we acknowledge that the inclusion of patient ID in the time series classification model too but a look into feature significances shows how important time is.
The models used can make an accurate prediction on sleep state given that window’s motion and heart rate, but the information from all the data preceding it is very important in maximising model performance and returning high accuracy of predictions.
K-Nearest Neighbours was the best performing algorithm for the extracted feature data across two, three and five-label datasets. Benefits of this include that K-NN algorithm is very simple to understand and equally easy to implement. To classify the new data point K-NN algorithm reads through whole dataset to find out K nearest neighbors and classifies that point with the mode of the neighbours’ labels. It’s suitable for this assignment due to its ease to adjust to multi-class problems. KNN has no capability of e</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>